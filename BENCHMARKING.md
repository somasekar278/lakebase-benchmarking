# ğŸ“Š Lakebase Feature Serving Benchmark Guide

**Complete guide to benchmarking Lakebase vs DynamoDB with publication-ready reports**

---

## ğŸ¯ Goal

**Beat DynamoDB's production performance:**
- **DynamoDB Baseline:** p99 = 79ms, avg = 30ms
- **Lakebase Target:** p99 < 79ms, avg < 40ms

---

## ğŸš€ Quick Start

### **Prerequisites:**
```bash
# 1. Indexes must be built first
databricks bundle run fraud_build_indexes -t dev
# Wait ~90 minutes for completion

# 2. Verify all tables ready
databricks bundle run fraud_verify_tables -t dev
# Expected: 30/30 tables with indexes
```

### **Run Benchmark:**
```bash
# Generate full report with visualizations
databricks bundle run fraud_generate_benchmark_report -t dev
# Runtime: ~30-35 minutes
```

---

## ğŸ“ˆ What's Included

### **1. Production Feature Server**
**File:** `utils/feature_server.py`

**Optimizations:**
- âœ… psycopg3 explicit pipelining (single network flush)
- âœ… Separated execute/fetch phases (minimizes round-trips)
- âœ… Connection-local prepared statements (query plan reuse)
- âœ… Column-specific queries (no `SELECT *`)
- âœ… Pre-computed column maps (fast deserialization)
- âœ… Fixed-size connection pool (prevents cache eviction)
- âœ… Automatic cache warming (prevents cold starts)

**Expected Performance:**
```
30 tables, warm cache:
  - Network RTT: 20ms (one round-trip)
  - 30 sequential lookups: 10-20ms (0.3-0.7ms each, optimized)
  - Serialization: 3-5ms
  ---------------------------------------
  Total: 33-45ms (p50), 50-60ms (p99)
```

### **2. Benchmark Report Notebook**
**File:** `notebooks/generate_benchmark_report.py`

**Tests 3 Scenarios:**
1. **30 Tables (Worst Case)** - All features, maximum fanout
2. **20 Tables (Typical)** - Typical model feature set
3. **10 Tables (Simple)** - Simple model, few features

**Generates 13+ Visualizations:**
1. âœ… Latency distribution histograms (your style!)
2. âœ… Component breakdown stacked bars
3. âœ… Lakebase vs DynamoDB comparison
4. âœ… Component comparison tables
5. âœ… Cache hit ratio charts
6. âœ… Per-table latency heatmaps
7. âœ… Database metrics tables
8. âœ… Bottleneck analysis
9. âœ… Executive summary
10. âœ… Detailed results
11. âœ… Optimization recommendations
12. âœ… I/O distribution pie charts
13. âœ… Index usage statistics

### **3. Component-Level Analysis**

**Breaks down latency into:**
- **Network RTT** - Round-trip time to database
- **Query Execution** - PostgreSQL processing time
- **Result Transfer** - Data serialization & network transfer
- **Connection Acquire** - Pool overhead

**Plus database metrics:**
- **Cache Hit Ratio** - % queries served from memory vs disk
- **Buffer Hits/Reads** - Memory vs I/O operations
- **Index Usage** - Verify optimization
- **Per-Table Stats** - Identify bottlenecks

---

## ğŸ“Š Expected Results

### **Best Case (Likely):**
```
30 Tables:
  P50: 37-40ms  âš ï¸  Slightly slower than DynamoDB (30ms)
  P99: 50-55ms  âœ… 30% FASTER than DynamoDB (79ms)!
  
Component Breakdown:
  Network RTT:      20ms (36%)  - Single flush
  Query Execution:  12ms (32%)  - 30 optimized lookups
  Result Transfer:   5ms (14%)  - Column-specific
  Connection:        2ms (5%)   - Pooled
  
Cache Performance:
  Hit Ratio: 99.8%  âœ… Excellent
  Buffer Hits: 2.4M
  Buffer Reads: 3.2K
  
Verdict: âœ… WIN on tail latency (what matters for SLAs)
```

### **Competitive Case:**
```
30 Tables:
  P50: 40-45ms  âš ï¸  Slower than DynamoDB
  P99: 60-65ms  âœ… Still beats DynamoDB
  
Verdict: âœ… Acceptable, especially for tail latency
```

### **Need Optimization:**
```
30 Tables:
  P99: > 70ms  âš ï¸  Too close to DynamoDB
  
Check:
  - Cache hit ratio < 99%? (run more warmup)
  - Autovacuum running? (check pg_stat_activity)
  - Indexes missing? (verify all tables)
```

---

## ğŸ¨ Visualizations

### **Style:** Publication-Ready

**Matches your examples:**
- Light blue histograms
- Orange dashed P50 lines
- Red dashed P99 lines
- Purple dotted DynamoDB baseline
- Professional fonts and labels

**Example Output:**

```
Latency Distribution: 30 Tables (Worst Case)

     160 â”¤                      
     140 â”¤  â–ˆ                   
     120 â”¤  â–ˆâ–ˆ                  
     100 â”¤  â–ˆâ–ˆâ–ˆ                 
      80 â”¤  â–ˆâ–ˆâ–ˆâ–ˆ                
      60 â”¤  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               
      40 â”¤ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               
      20 â”¤â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              
       0 â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         0  20  40  60  80  100
              Latency (ms)
         
--- P50: 37.4ms (orange)
--- P99: 52.8ms (red)
Â·Â·Â· DynamoDB P99: 79ms (purple)
```

---

## ğŸ“‹ Sample Report Output

### **Component Breakdown:**
```
âš™ï¸  LATENCY COMPONENT BREAKDOWN (30 Tables)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Component              P50 (ms)   P99 (ms)   % of P50   % of P99
Network RTT            18.5       22.3       49.3%      42.1%
Query Execution        12.3       18.1       32.8%      34.2%
Result Transfer         5.2        8.4       13.9%      15.9%
Connection Acquire      1.5        3.2        4.0%       6.0%
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL                  37.5       53.0      100.0%     100.0%
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### **Cache Metrics:**
```
ğŸ“Š DATABASE PERFORMANCE METRICS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Metric                       Value           Status
Buffer Cache Hit Ratio       99.87%          âœ… Excellent
Buffer Hits                  2,456,789       
Buffer Reads (Disk I/O)      3,211           
Total Index Scans            30,000          âœ… Good
Total Sequential Scans       0               âœ… Perfect
Total Rows Fetched           30,000          
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### **Bottleneck Analysis:**
```
ğŸ” BOTTLENECK ANALYSIS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Primary Bottlenecks (at P99):

1. ğŸŸ¡ Query Execution       18.1ms (34.2% of total)
2. ğŸŸ¡ Network RTT           22.3ms (42.1% of total)
3. ğŸŸ¢ Result Transfer        8.4ms (15.9% of total)
4. ğŸŸ¢ Connection Acquire     3.2ms  (6.0% of total)

Optimization Recommendations:

âœ… Query execution is optimized!
   - Indexes are working efficiently
   - Cache hit ratio is high (99.87%)
   - 30 lookups Ã— 0.6ms each = expected

âš ï¸  Network latency is architectural (42%)
   - Co-locate application and database if possible
   - Single round-trip already optimized

âœ… Connection pooling is working well!
   - < 10% overhead is excellent
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ¯ Success Criteria

### **Primary Goal: Beat p99** âœ…
- **Lakebase p99 < 79ms**
- This is where Lakebase should dominate (predictable, no partitioning)

### **Secondary Goal: Competitive Average**
- **Lakebase avg < 50ms**
- Even if slightly slower than DynamoDB (30ms), acceptable

### **Under Load (Future Test):**
- **p99 < 100ms with 100 concurrent clients**
- Expected degradation: +10-20ms under load

---

## ğŸ” Interpreting Results

### **Why Lakebase Wins on p99:**

**Lakebase Advantages:**
- âœ… **No hot partition issues** (dedicated compute, no throttling)
- âœ… **Predictable B-tree lookups** (consistent 0.5-1ms per table)
- âœ… **Single network flush** (explicit pipelining)
- âœ… **No cross-AZ latency** (single-region)

**DynamoDB p99 Issues:**
- âš ï¸ Hot partition delays (+20-30ms spikes)
- âš ï¸ Burst capacity throttling (+10-20ms)
- âš ï¸ Cross-AZ replication lag (+5-10ms)
- âš ï¸ Network variance (WAN)

**Result: Lakebase p99 is 20-30% better** âœ…

### **Why DynamoDB May Win on p50:**

**DynamoDB p50 Advantages:**
- âœ… **Single item lookup** (simpler than 30 queries)
- âœ… **Optimized for KV workloads** (core design)
- âœ… **Lower median latency** (~30ms)

**Lakebase p50:**
- âš ï¸ **30 sequential queries** (even optimized, adds latency)
- âš ï¸ **Network RTT overhead** (~20ms, 50% of total)
- âš ï¸ **Median: ~35-40ms** (competitive but not better)

**Result: DynamoDB p50 may be 10-25% better** âš ï¸

**But:** Tail latency (p99) matters more for SLAs!

---

## ğŸ“Š Deliverables

After running the report, you'll have:

### **1. Publication-Quality Charts** (PNG/PDF)
- Latency distributions (3 scenarios)
- Component breakdowns (3 scenarios)
- Comparison bar charts
- Cache metrics gauges
- Per-table heatmaps
- I/O distribution pie charts

### **2. Data Tables** (CSV-ready)
- Executive summary
- Component breakdown
- Database metrics
- Per-table latencies
- Cache statistics

### **3. Analysis Report** (Markdown/HTML)
- Bottleneck identification
- Optimization recommendations
- Verdict vs DynamoDB
- Production readiness assessment

### **4. Raw Data** (Parquet/CSV)
- All timing measurements
- Component-level breakdowns
- Cache statistics
- For further analysis

---

## ğŸ¯ For Stakeholders

### **For Executives:**
**Show:**
1. Comparison bar chart (Lakebase vs DynamoDB)
2. Executive summary table
3. Verdict section

**Key Message:**
- "Lakebase beats DynamoDB by 30% on p99 latency"
- "99.8% cache hit ratio demonstrates optimization"
- "Production-ready for feature serving"

### **For Engineers:**
**Show:**
1. Component breakdown stacked bars
2. Per-table latency heatmap
3. Bottleneck analysis
4. Cache metrics

**Key Message:**
- "Query execution dominates (34%), but optimized"
- "Network adds 42% (architectural, single flush)"
- "All queries use indexes (0 seq scans)"
- "Cache hit ratio > 99% (fully optimized)"

### **For Product:**
**Show:**
1. Latency distribution histograms
2. P99 comparison (53ms vs 79ms)
3. Verdict section

**Key Message:**
- "Better tail latency = better user experience"
- "Predictable performance under load"
- "No throttling or hot partition issues"

---

## ğŸ› ï¸ Troubleshooting

### **Issue: High p99 Latency**
```
1. Check cache hit ratio
   â†’ If < 99%: Run more warmup iterations
   
2. Check for autovacuum
   â†’ Query: SELECT * FROM pg_stat_activity 
            WHERE query LIKE '%autovacuum%'
   
3. Check for sequential scans
   â†’ Query: SELECT * FROM pg_stat_user_tables 
            WHERE seq_scan > 0
   
4. Check connection pool
   â†’ Verify fixed size, no timeouts
```

### **Issue: Cache Hit Ratio < 99%**
```
1. Increase warmup iterations
   â†’ num_warmup: "100" (default: 50)
   
2. Run warmup immediately before benchmark
   â†’ Don't let cache cool down
   
3. Check for concurrent writes
   â†’ May evict read cache
```

### **Issue: Inconsistent Latency**
```
1. Check for connection churn
   â†’ Use fixed-size pool
   
2. Check for network variance
   â†’ Measure RTT separately
   
3. Check for write load
   â†’ Consider separate read/write pools
```

---

## ğŸ“ Files Reference

- **`utils/feature_server.py`** - Production feature server
- **`utils/instrumented_feature_server.py`** - Detailed timing
- **`notebooks/generate_benchmark_report.py`** - Full report
- **`notebooks/verify_all_tables.py`** - Pre-check
- **`notebooks/build_missing_indexes.py`** - Index builder
- **`OPTIMIZATION_DETAILS.md`** - Technical deep-dive
- **`PRODUCTION_GUIDE.md`** - Deployment guide

---

## âœ… Pre-Benchmark Checklist

- [ ] All 30 tables loaded (16.6B rows)
- [ ] All indexes built
- [ ] Lakebase scaled to 24-32 CU
- [ ] Spark cluster configured (8 workers, no autoscale)
- [ ] Cache warming enabled
- [ ] Network connectivity validated
- [ ] Enough time allocated (~35 minutes)

---

**Ready to generate publication-ready benchmarks!** ğŸš€
