# V5.4 Pool Wait Time Fix - Complete Implementation

## Summary
Fixed parallel mode latency measurement to include pool wait time (wall-clock user-perceived latency) instead of DB-only time. Added surgical Gantt chart segmentation to visualize both pool wait and DB execution.

---

## Changes Applied

### ✅ Change 1: Return pool wait times as dict (paired with entities)

**Problem:** `pool_wait_times` was a list, unsafe for out-of-order future completion.

**Solution:** Changed to `entity_pool_waits` dict keyed by entity name.

```python
# Before
pool_wait_times = []
pool_wait_times.append(pool_wait_ms)
return ..., pool_wait_times

# After
entity_pool_waits = {}
entity_pool_waits[entity] = pool_wait_ms
return ..., entity_pool_waits
```

---

### ✅ Change 2: Compute wall-clock request latency for parallel mode

**Problem:** Reported `request_latency_ms = max(entity_timings.values())` excluded pool wait time.

**Solution:** Added pool wait to each entity's timing, then took max.

```python
# Before
request_latency_ms = max(entity_timings.values()) if entity_timings else 0

# After
if entity_timings:
    entity_wall_clock_times = {
        entity: entity_timings[entity] + entity_pool_waits.get(entity, 0)
        for entity in entity_timings
    }
    request_latency_ms = max(entity_wall_clock_times.values())
else:
    request_latency_ms = 0
```

**Also updated:** Unified all modes to use canonical `request_latency_ms` for `latencies.append()` to ensure consistency.

---

### ✅ Change 3: Increase pool min_size with guard

**Problem:** With `min_size = min(workers*2, 10)`, parallel(1 worker) had only 2 connections for 3 entities, causing connection creation overhead (210-246ms delays).

**Solution:** Pre-warm at least 6 connections, with guard to never exceed max_size.

```python
# Before
min_size = min(current_workers * 2, 10)

# After
pool_min_size = max(6, current_workers * 2)
pool_min_size = min(pool_min_size, pool_max_size)  # Guard: never exceed max_size
```

**Result:** For parallel(1 worker): min=6, max=10 (sufficient for 3 entities + buffer)

---

### ✅ Change 4: Gantt chart segmentation (surgical)

**Problem:** Should Gantt show wall-clock or DB-only? Both have value.

**Solution:** Show BOTH as separate segments per entity.

#### Parallel mode (2 segments per entity):
```python
gantt_segments = [
    {
        "entity": entity_info["entity"],
        "segment": "pool_wait",  # ← NEW segment type
        "start_ms": (pool_wait_start - request_start) * 1000,
        "end_ms": (entity_start - request_start) * 1000
    },
    {
        "entity": entity_info["entity"],
        "segment": "db_exec",
        "start_ms": (entity_start - request_start) * 1000,
        "end_ms": (entity_end - request_start) * 1000
    }
]
```

#### Serial/Binpacked modes (1 segment per entity):
```python
gantt_data.append({
    "entity": entity_info["entity"],
    "segment": "db_exec",  # ← Consistent structure (no pool wait)
    "start_ms": (entity_start - request_start) * 1000,
    "end_ms": (entity_end - request_start) * 1000
})
```

**Visualization guidance (for `zipfian_benchmark_visuals.py`):**
- Same color family, but lighter shade for `pool_wait`, darker for `db_exec`
- Or use hatch pattern for `pool_wait`
- Legend: "Pool wait" and "DB execution"
- Title: "Request timeline (includes pool wait + DB execution)"

---

## Impact

### Before Fix:
```
Parallel (1 worker) at 0% hot:
  Reported P99: 65ms (DB-only)
  Actual P99:   65ms + 246ms = 311ms (wall-clock)
  ⚠️ Misleading - looks 4.7x better than reality!
```

### After Fix:
```
Parallel (1 worker) at 0% hot:
  Reported P99: ~311ms (wall-clock)
  ✅ Honest measurement of user-perceived latency
  ✅ Gantt shows both pool wait (246ms) and DB exec (65ms)
```

---

## Validation Checklist

- ✅ `entity_pool_waits` returned as dict (safe for out-of-order futures)
- ✅ `request_latency_ms` includes pool wait for parallel mode
- ✅ `latencies.append(request_latency_ms)` uses canonical value
- ✅ Pool `min_size` pre-warmed to 6+ connections
- ✅ Guard: `min_size = min(min_size, max_size)`
- ✅ Gantt segments: parallel has 2 per entity, serial/binpacked has 1
- ✅ All gantt entries have consistent `segment` field

---

## Files Modified

- `notebooks/benchmark_zipfian_realistic_v5.4.py`
  - `fetch_entity_worker()` - Returns 2 Gantt segments
  - `fetch_features_binpacked_parallel()` - Returns dict, extends gantt_segments
  - Pool initialization - Increased min_size
  - Main loop - Computes wall-clock latency, uses canonical `request_latency_ms`
  - Serial/binpacked - Added `segment: "db_exec"` for consistency

---

## Next Steps

1. **Visualization update (separate task):**
   - Update `zipfian_benchmark_visuals.py` to render Gantt with both segments
   - Use lighter shade for `pool_wait`, darker for `db_exec`
   - Add legend and update chart title

2. **Current run:**
   - Let the current benchmark finish (already running with old code)
   - Results will still be valid but parallel mode P99 will be DB-only

3. **Next run:**
   - Sync updated notebook to Databricks
   - Run with fix applied
   - Parallel mode will show honest wall-clock latency

---

## Technical Notes

### Why dict over list for pool_wait_times?
Futures complete out-of-order. With a list, `pool_wait_times[0]` may not correspond to `entity_timings['card_fingerprint']`. A dict ensures correct pairing.

### Why 6 connections minimum?
- 3 entities run in parallel
- Each needs 1 connection
- Buffer of 3 for connection recycling/overhead
- Avoids SSL handshake delays (~100-300ms per new connection)

### Why wall-clock instead of DB-only?
User-perceived latency = pool wait + DB execution. Reporting DB-only makes parallel mode look artificially better than it performs in production.

---

## Commit Message (when ready)

```
Fix parallel mode to include pool wait time in latency (V5.4)

Problem:
- Parallel mode reported DB-only latency (excluded pool acquisition time)
- Made parallel(1 worker) look 4.7x better than reality (65ms vs 311ms)
- Pool contention with min_size=2 for 3 entities caused 200ms+ delays

Solution:
1. Return entity_pool_waits as dict (safe for out-of-order futures)
2. Compute wall-clock request_latency_ms (pool_wait + db_exec)
3. Increase pool min_size to max(6, workers*2) with guard
4. Gantt: return 2 segments per entity (pool_wait + db_exec)

Impact:
- Parallel mode now reports honest user-perceived latency
- Gantt visualizes both pool wait and DB execution separately
- Serial/binpacked unchanged (no pool contention)
```
