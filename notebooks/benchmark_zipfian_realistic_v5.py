# Databricks notebook source
# MAGIC %md
# MAGIC # üéØ Realistic Zipfian Feature Serving Benchmark V3 (Multi-Entity)
# MAGIC 
# MAGIC **Production-Grade Benchmark with:**
# MAGIC - ‚úÖ Random key sampling from ALL tables (not just first keys)
# MAGIC - ‚úÖ SELECT * (fetches actual columns, not just index)
# MAGIC - ‚úÖ Serial execution (realistic per-query latency)
# MAGIC - ‚úÖ Per-entity timing (independent measurement)
# MAGIC - ‚úÖ Aggregate I/O tracking (via pg_statio_user_tables)
# MAGIC - ‚úÖ Key persistence for reproducibility
# MAGIC 
# MAGIC **Request Structure:**
# MAGIC - Each request = 3 entities (card_fingerprint, customer_email, cardholder_name)
# MAGIC - Each entity fans out to 9-12 tables (30 tables total per request)
# MAGIC - Hot/cold determined independently per entity
# MAGIC - Latency = SUM(entity latencies) for serial execution

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1Ô∏è‚É£ Install Dependencies

# COMMAND ----------

%pip install psycopg[binary,pool] numpy pandas matplotlib seaborn
dbutils.library.restartPython()

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2Ô∏è‚É£ Configuration

# COMMAND ----------

dbutils.widgets.text("lakebase_host", "", "Lakebase Host")
dbutils.widgets.text("lakebase_database", "benchmark", "Database")
dbutils.widgets.text("lakebase_schema", "features", "Schema")
dbutils.widgets.text("lakebase_user", "", "User")
dbutils.widgets.text("lakebase_password", "", "Password")
dbutils.widgets.text("iterations_per_run", "1000", "Iterations Per Run")
dbutils.widgets.text("total_keys_per_entity", "10000", "Total Keys Per Entity")
dbutils.widgets.text("hot_key_percent", "1", "Hot Key % of Dataset")
dbutils.widgets.text("explain_sample_rate", "999999", "EXPLAIN Sample Rate (1 in N, 999999=disabled)")
dbutils.widgets.dropdown("run_all_modes", "true", ["true", "false"], "Run All Modes Sequentially")
dbutils.widgets.dropdown("fetch_mode", "serial", ["serial", "binpacked", "binpacked_parallel"], "Fetch Mode (if run_all_modes=false)")
dbutils.widgets.text("reuse_run_id", "", "Reuse Keys from Run ID (empty=sample new)")
dbutils.widgets.text("parallel_workers", "1,2,3,4", "Parallel Workers (comma-separated)")
dbutils.widgets.dropdown("log_query_timings", "true", ["true", "false"], "üÜï V5.1: Log Slow Queries")
dbutils.widgets.text("slow_query_threshold_ms", "40", "üÜï V5.1: Slow Query Threshold (ms)")

LAKEBASE_CONFIG = {
    "host": dbutils.widgets.get("lakebase_host"),
    "port": 5432,
    "dbname": dbutils.widgets.get("lakebase_database"),
    "user": dbutils.widgets.get("lakebase_user"),
    "password": dbutils.widgets.get("lakebase_password"),
    "sslmode": "require",
    # ‚úÖ CRITICAL: Prevent connection timeouts during long-running benchmarks
    "connect_timeout": 300,  # 5 minutes to establish connection
    "keepalives": 1,  # Enable TCP keepalive
    "keepalives_idle": 30,  # Send keepalive after 30s of idle
    "keepalives_interval": 10,  # Resend keepalive every 10s
    "keepalives_count": 5,  # Give up after 5 failed keepalives
    "options": "-c statement_timeout=0 -c idle_in_transaction_session_timeout=0"  # No query/idle timeouts
}

SCHEMA = dbutils.widgets.get("lakebase_schema")
ITERATIONS_PER_RUN = int(dbutils.widgets.get("iterations_per_run"))
TOTAL_KEYS_PER_ENTITY = int(dbutils.widgets.get("total_keys_per_entity"))
HOT_KEY_PERCENT_OF_DATASET = int(dbutils.widgets.get("hot_key_percent"))
EXPLAIN_SAMPLE_RATE = int(dbutils.widgets.get("explain_sample_rate"))
RUN_ALL_MODES = dbutils.widgets.get("run_all_modes") == "true"
FETCH_MODE = dbutils.widgets.get("fetch_mode")  # serial | binpacked | binpacked_parallel
REUSE_RUN_ID = dbutils.widgets.get("reuse_run_id").strip()  # Empty string = sample new keys
PARALLEL_WORKERS_STR = dbutils.widgets.get("parallel_workers")  # e.g., "1,2,3,4"
PARALLEL_WORKERS = [int(w.strip()) for w in PARALLEL_WORKERS_STR.split(",")]
LOG_QUERY_TIMINGS = dbutils.widgets.get("log_query_timings") == "true"  # V5.1: Log slow queries
SLOW_QUERY_THRESHOLD_MS = float(dbutils.widgets.get("slow_query_threshold_ms"))  # V5.1: Threshold for slow query log

# Hot/cold matrix to test
HOT_COLD_MATRIX = [100, 90, 80, 70, 60, 50, 30, 10, 0]

# ‚úÖ V5: Build mode configurations with worker counts
MODE_CONFIGS = []

if RUN_ALL_MODES:
    # Serial and bin-packed (no workers)
    MODE_CONFIGS.append({"mode": "serial", "workers": None})
    MODE_CONFIGS.append({"mode": "binpacked", "workers": None})
    
    # Parallel with concurrency sweep
    for workers in PARALLEL_WORKERS:
        MODE_CONFIGS.append({"mode": "binpacked_parallel", "workers": workers})
else:
    # Single mode
    if FETCH_MODE == "binpacked_parallel":
        for workers in PARALLEL_WORKERS:
            MODE_CONFIGS.append({"mode": FETCH_MODE, "workers": workers})
    else:
        MODE_CONFIGS.append({"mode": FETCH_MODE, "workers": None})

# Mode descriptions
def get_mode_label(mode, workers):
    if mode == "serial":
        return "Serial (30 queries, baseline)"
    elif mode == "binpacked":
        return "Bin-packed (10 UNION ALL queries, serial)"
    elif mode == "binpacked_parallel":
        return f"Bin-packed + Parallel (10 queries, {workers} workers)"
    return mode

# Mode labels for summary display
FETCH_MODE_LABELS = {
    "serial": "Serial (30 queries, baseline)",
    "binpacked": "Bin-packed (10 UNION ALL queries, serial)",
    "binpacked_parallel": "Bin-packed + Parallel (10 queries)"
}

print("="*80)
print("‚öôÔ∏è  MULTI-ENTITY ZIPFIAN BENCHMARK V5 CONFIGURATION")
print("="*80)
print(f"Lakebase:               {LAKEBASE_CONFIG['host']}")
print(f"Schema:                 {SCHEMA}")
print()
print(f"üöÄ V5 ENHANCEMENTS:")
print(f"   ‚úÖ Cost-normalized metrics (latency per query)")
print(f"   ‚úÖ Concurrency sweep (workers: {PARALLEL_WORKERS})")
print(f"   ‚úÖ Entity timing details (Gantt chart support)")
print()
print(f"üìä MODE CONFIGURATIONS ({len(MODE_CONFIGS)} total):")
for cfg in MODE_CONFIGS:
    print(f"   ‚Ä¢ {get_mode_label(cfg['mode'], cfg['workers'])}")
print()
print(f"üîë KEY SAMPLING:")
print(f"   Total keys/entity:   {TOTAL_KEYS_PER_ENTITY:,}")
print(f"   Hot key %:           {HOT_KEY_PERCENT_OF_DATASET}% (randomly selected)")
print(f"   Hot keys/entity:     {int(TOTAL_KEYS_PER_ENTITY * HOT_KEY_PERCENT_OF_DATASET / 100):,}")
print()
print(f"üìä BENCHMARK SETTINGS:")
print(f"   Iterations/run:      {ITERATIONS_PER_RUN:,}")
print(f"   Hot/cold ratios:     {HOT_COLD_MATRIX}")
print(f"   EXPLAIN sample:      1 in {EXPLAIN_SAMPLE_RATE} queries (precise I/O)")
print()
print(f"üéØ REQUEST STRUCTURE:")
print(f"   Entities/request:    3 (card, email, name)")
print(f"   Tables/request:      30 (9-12 tables per entity)")
print(f"   Hot/cold:            Independent per entity")
print()
print(f"‚úÖ IMPROVEMENTS:")
print(f"   ‚Ä¢ Random key sampling from ALL tables (not just first keys)")
print(f"   ‚Ä¢ SELECT * (fetches actual data, not just index)")
print(f"   ‚Ä¢ EXPLAIN sampling for precise I/O measurement")
print(f"   ‚Ä¢ Error handling with graceful degradation")
print(f"   ‚Ä¢ NaN-safe correlation calculation")
print(f"   ‚Ä¢ TABLESAMPLE fallback for small tables")
print("="*80)

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3Ô∏è‚É£ Entity Table Groups

# COMMAND ----------

# Entity ‚Üí tables fanout (matches DynamoDB BatchGetItem grouping)
ENTITY_TABLE_GROUPS = {
    "card_fingerprint": [
        "client_id_card_fingerprint__fraud_rates__30d",
        "client_id_card_fingerprint__fraud_rates__90d",
        "client_id_card_fingerprint__fraud_rates__365d",
        "client_id_card_fingerprint__time_since__30d",
        "client_id_card_fingerprint__time_since__90d",
        "client_id_card_fingerprint__time_since__365d",
        "client_id_card_fingerprint__good_rates__30d",
        "client_id_card_fingerprint__good_rates__90d",
        "client_id_card_fingerprint__good_rates__365d",
    ],
    "customer_email": [
        "client_id_customer_email_clean__fraud_rates__30d",
        "client_id_customer_email_clean__fraud_rates__90d",
        "client_id_customer_email_clean__fraud_rates__365d",
        "client_id_customer_email_clean__time_since__30d",
        "client_id_customer_email_clean__time_since__90d",
        "client_id_customer_email_clean__time_since__365d",
        "client_id_customer_email_clean__good_rates__30d",
        "client_id_customer_email_clean__good_rates__90d",
        "client_id_customer_email_clean__good_rates__365d",
    ],
    "cardholder_name": [
        "client_id_cardholder_name_clean__fraud_rates__30d",
        "client_id_cardholder_name_clean__fraud_rates__90d",
        "client_id_cardholder_name_clean__fraud_rates__365d",
        "client_id_cardholder_name_clean__tesseract_velocities__30d",
        "client_id_cardholder_name_clean__tesseract_velocities__90d",
        "client_id_cardholder_name_clean__tesseract_velocities__365d",
        "client_id_cardholder_name_clean__time_since__30d",
        "client_id_cardholder_name_clean__time_since__90d",
        "client_id_cardholder_name_clean__time_since__365d",
        "client_id_cardholder_name_clean__good_rates__30d",
        "client_id_cardholder_name_clean__good_rates__90d",
        "client_id_cardholder_name_clean__good_rates__365d",
    ],
}

ENTITY_NAMES = list(ENTITY_TABLE_GROUPS.keys())

print(f"‚úÖ Entity groups loaded:")
for entity, tables in ENTITY_TABLE_GROUPS.items():
    print(f"   {entity:20} ‚Üí {len(tables):2} tables")
print(f"\nTotal tables per request: {sum(len(t) for t in ENTITY_TABLE_GROUPS.values())}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 4Ô∏è‚É£ Import Libraries

# COMMAND ----------

import time
import random
import psycopg
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import json
import uuid
import re
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor
from psycopg_pool import ConnectionPool

# Generate unique run ID
RUN_ID = str(uuid.uuid4())[:8]
RESULTS_SCHEMA = SCHEMA
RESULTS_TABLE = "zipfian_feature_serving_results_v5"
KEYS_TABLE = "zipfian_keys_per_run"

# Plotting style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (16, 10)

print("‚úÖ Libraries imported")
print(f"üìã Run ID: {RUN_ID}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 5Ô∏è‚É£ Database Connection & Helpers

# COMMAND ----------

def get_conn():
    """
    Get PostgreSQL connection with keepalive enabled
    
    Returns:
        psycopg.Connection with aggressive keepalive settings
    """
    conn = psycopg.connect(**LAKEBASE_CONFIG)
    
    # Verify connection is alive
    try:
        with conn.cursor() as cur:
            cur.execute("SELECT 1")
            cur.fetchone()
    except Exception as e:
        print(f"‚ö†Ô∏è  Connection test failed: {e}")
        raise
    
    return conn

def refresh_connection_if_stale(conn):
    """
    Check if connection is healthy and refresh if needed
    
    Args:
        conn: Current connection
    
    Returns:
        Connection (either existing or new)
    """
    try:
        # Quick health check
        with conn.cursor() as cur:
            cur.execute("SELECT 1")
            cur.fetchone()
        return conn
    except Exception as e:
        print(f"      ‚ö†Ô∏è  Connection unhealthy ({e}), refreshing...")
        try:
            conn.close()
        except:
            pass
        return get_conn()

def fetch_sample_keys(conn, table, limit):
    """‚úÖ FIXED: Fetch random sample of keys with TABLESAMPLE + fallback"""
    with conn.cursor() as cur:
        # Try TABLESAMPLE SYSTEM first (fast for large tables)
        cur.execute(f"""
            SELECT DISTINCT hash_key 
            FROM {SCHEMA}.{table} 
            TABLESAMPLE SYSTEM (1)
            LIMIT %s
        """, (limit,))
        keys = [r[0] for r in cur.fetchall()]
        
        # Fallback to ORDER BY RANDOM() if insufficient keys
        if len(keys) < limit * 0.8:  # Less than 80% of requested
            print(f"         ‚ö†Ô∏è  TABLESAMPLE returned only {len(keys)} keys, using ORDER BY RANDOM()")
            cur.execute(f"""
                SELECT DISTINCT hash_key 
                FROM {SCHEMA}.{table} 
                ORDER BY RANDOM()
                LIMIT %s
            """, (limit,))
            keys = [r[0] for r in cur.fetchall()]
        
        return keys

def reset_pg_stats(conn):
    """Reset PostgreSQL I/O statistics"""
    try:
        with conn.cursor() as cur:
            cur.execute("SELECT pg_stat_reset();")
            conn.commit()
        return True
    except Exception as e:
        print(f"‚ö†Ô∏è  Could not reset stats (requires superuser): {e}")
        conn.rollback()
        return False

def flush_cache(conn):
    """
    Flush PostgreSQL shared buffers and OS cache
    
    Note: This requires superuser privileges and may not work in all environments.
    We do our best to flush what we can.
    """
    print("üîÑ Flushing caches...")
    
    # Try to discard PostgreSQL shared buffers
    try:
        with conn.cursor() as cur:
            # This extension allows flushing specific tables
            # If not available, we'll just note it
            cur.execute("SELECT pg_stat_reset();")
            conn.commit()
        print("   ‚úÖ PostgreSQL stats reset")
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Could not reset PostgreSQL stats: {e}")
        conn.rollback()
    
    # Note: Full OS cache flush requires system-level commands (not available in PostgreSQL)
    # The best we can do is:
    # 1. Close and reopen connections (done between modes)
    # 2. Query different keys (we use same keys for fairness)
    # 3. Run VACUUM ANALYZE to update stats
    
    try:
        with conn.cursor() as cur:
            for entity, tables in ENTITY_TABLE_GROUPS.items():
                for table in tables:
                    cur.execute(f"VACUUM ANALYZE {SCHEMA}.{table}")
        conn.commit()
        print("   ‚úÖ VACUUM ANALYZE completed on all tables")
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Could not run VACUUM ANALYZE: {e}")
        conn.rollback()
    
    print("   ‚ÑπÔ∏è  Note: Full OS cache flush not possible from PostgreSQL")
    print("   ‚ÑπÔ∏è  Cache behavior will still be realistic due to cold key access")

def read_pg_io_stats(conn):
    """Read PostgreSQL I/O statistics for this schema"""
    with conn.cursor() as cur:
        cur.execute("""
            SELECT 
                COALESCE(SUM(heap_blks_read), 0) as reads,
                COALESCE(SUM(heap_blks_hit), 0) as hits
            FROM pg_statio_user_tables
            WHERE schemaname = %s
        """, (SCHEMA,))
        return cur.fetchone()

def parse_buffers_from_explain(explain_output):
    """
    Extract shared blocks read and planning time from EXPLAIN (ANALYZE, BUFFERS) output
    ‚úÖ V5.1: Enhanced to return full explain data for slow query log
    """
    buffers_read = 0
    buffers_hit = 0
    planning_time_ms = 0
    execution_time_ms = 0
    
    for line in explain_output:
        text = str(line[0])
        
        # Parse "Buffers: shared hit=8 read=152"
        if "Buffers:" in text:
            read_match = re.search(r'read=(\d+)', text)
            if read_match:
                buffers_read = int(read_match.group(1))
            hit_match = re.search(r'hit=(\d+)', text)
            if hit_match:
                buffers_hit = int(hit_match.group(1))
        
        # Parse "Planning Time: 0.123 ms"
        if "Planning Time:" in text:
            match = re.search(r'Planning Time:\s+([\d.]+)\s+ms', text)
            if match:
                planning_time_ms = float(match.group(1))
        
        # Parse "Execution Time: 1.234 ms"
        if "Execution Time:" in text:
            match = re.search(r'Execution Time:\s+([\d.]+)\s+ms', text)
            if match:
                execution_time_ms = float(match.group(1))
    
    # Return tuple for backward compatibility + dict for V5.1
    explain_data = {
        "shared_blks_read": buffers_read,
        "shared_blks_hit": buffers_hit,
        "planning_ms": planning_time_ms,
        "execution_ms": execution_time_ms
    }
    
    return buffers_read, planning_time_ms, explain_data

print("‚úÖ Database helper functions loaded")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 6Ô∏è‚É£ Results Table Setup

# COMMAND ----------

def ensure_results_table(conn):
    """Create results and keys tables if they don't exist"""
    with conn.cursor() as cur:
        # Results table
        cur.execute(f"""
            CREATE TABLE IF NOT EXISTS {RESULTS_SCHEMA}.{RESULTS_TABLE} (
                run_id                  TEXT,
                run_ts                  TIMESTAMP DEFAULT now(),

                hot_traffic_pct         INT,
                cold_traffic_pct        INT,

                iterations              INT,
                p50_ms                  DOUBLE PRECISION,
                p95_ms                  DOUBLE PRECISION,
                p99_ms                  DOUBLE PRECISION,
                avg_ms                  DOUBLE PRECISION,

                avg_cache_score         DOUBLE PRECISION,
                p50_cache_score         DOUBLE PRECISION,
                p90_cache_score         DOUBLE PRECISION,

                fully_cold_request_pct  DOUBLE PRECISION,
                fully_hot_request_pct   DOUBLE PRECISION,

                latency_cache_corr      DOUBLE PRECISION,
                io_blocks_per_request   DOUBLE PRECISION,

                entity_p99_ms           JSONB,

                notes                   TEXT
            );
        """)
        
        cur.execute(f"""
            CREATE INDEX IF NOT EXISTS idx_zipfian_run
            ON {RESULTS_SCHEMA}.{RESULTS_TABLE} (run_id, hot_traffic_pct);
        """)
        
        # Keys table - stores hot/cold keys for each run
        cur.execute(f"""
            CREATE TABLE IF NOT EXISTS {RESULTS_SCHEMA}.{KEYS_TABLE} (
                run_id      TEXT,
                entity_name TEXT,
                key_type    TEXT,  -- 'hot' or 'cold'
                hash_key    TEXT
            );
        """)
        
        cur.execute(f"""
            CREATE INDEX IF NOT EXISTS idx_keys_run
            ON {RESULTS_SCHEMA}.{KEYS_TABLE} (run_id, entity_name, key_type);
        """)
        
        # ‚úÖ V5.1: Slow query log for tail amplification analysis
        cur.execute(f"""
            CREATE TABLE IF NOT EXISTS {RESULTS_SCHEMA}.zipfian_slow_query_log (
                run_id               TEXT,
                mode                 TEXT,          -- serial / binpacked / parallel
                hot_traffic_pct      INT,
                iteration_id         INT,
                request_id           TEXT,

                entity_name          TEXT,          -- card_fingerprint / customer_email / cardholder_name
                table_name           TEXT,

                hash_key             TEXT,
                was_hot_key          BOOLEAN,

                query_latency_ms     DOUBLE PRECISION,
                rows_returned        INT,

                -- optional but super useful
                explain_used         BOOLEAN,
                shared_blks_read     INT,
                shared_blks_hit      INT,
                planning_ms          DOUBLE PRECISION,
                execution_ms         DOUBLE PRECISION,

                ts                   TIMESTAMP DEFAULT now()
            );
        """)
        
        cur.execute(f"""
            CREATE INDEX IF NOT EXISTS idx_slow_query_log_run
            ON {RESULTS_SCHEMA}.zipfian_slow_query_log (run_id, mode, hot_traffic_pct);
        """)
        
        conn.commit()
    print(f"‚úÖ Results table ensured: {RESULTS_SCHEMA}.{RESULTS_TABLE}")
    print(f"‚úÖ Slow query log table ensured: {RESULTS_SCHEMA}.zipfian_slow_query_log (V5.1)")
    print(f"‚úÖ Keys table ensured: {RESULTS_SCHEMA}.{KEYS_TABLE}")

def persist_results(conn, hot_pct, results):
    """Persist benchmark results to Lakebase"""
    
    # Try to add new columns if they don't exist (for backward compatibility)
    try:
        with conn.cursor() as cur:
            cur.execute(f"""
                ALTER TABLE {RESULTS_SCHEMA}.{RESULTS_TABLE}
                ADD COLUMN IF NOT EXISTS fetch_mode TEXT,
                ADD COLUMN IF NOT EXISTS queries_per_request INT,
                ADD COLUMN IF NOT EXISTS io_measurement_method TEXT,
                ADD COLUMN IF NOT EXISTS entity_p99_contribution_pct JSONB,
                ADD COLUMN IF NOT EXISTS cache_state TEXT,
                ADD COLUMN IF NOT EXISTS avg_planning_time_ms DOUBLE PRECISION,
                ADD COLUMN IF NOT EXISTS avg_rows_per_request DOUBLE PRECISION,
                ADD COLUMN IF NOT EXISTS avg_payload_bytes DOUBLE PRECISION,
                ADD COLUMN IF NOT EXISTS parallel_workers INT,
                ADD COLUMN IF NOT EXISTS latency_per_query_ms DOUBLE PRECISION,
                ADD COLUMN IF NOT EXISTS entity_timing_detail JSONB,
                ADD COLUMN IF NOT EXISTS max_concurrent_queries INT
            """)
            conn.commit()
    except:
        conn.rollback()
    
    with conn.cursor() as cur:
        cur.execute(f"""
            INSERT INTO {RESULTS_SCHEMA}.{RESULTS_TABLE} (
                run_id,
                hot_traffic_pct,
                cold_traffic_pct,
                iterations,
                p50_ms,
                p95_ms,
                p99_ms,
                avg_ms,
                avg_cache_score,
                p50_cache_score,
                p90_cache_score,
                fully_cold_request_pct,
                fully_hot_request_pct,
                latency_cache_corr,
                io_blocks_per_request,
                io_measurement_method,
                entity_p99_ms,
                entity_p99_contribution_pct,
                cache_state,
                fetch_mode,
                queries_per_request,
                avg_planning_time_ms,
                avg_rows_per_request,
                avg_payload_bytes,
                parallel_workers,
                latency_per_query_ms,
                entity_timing_detail,
                max_concurrent_queries,
                notes
            ) VALUES (
                %s, %s, %s, %s,
                %s, %s, %s, %s,
                %s, %s, %s,
                %s, %s,
                %s, %s, %s,
                %s, %s,
                %s,
                %s, %s,
                %s, %s, %s,
                %s, %s, %s, %s,
                %s
            )
        """, (
            RUN_ID,
            hot_pct,
            100 - hot_pct,
            ITERATIONS_PER_RUN,
            results["p50"],
            results["p95"],
            results["p99"],
            results["avg"],
            results["cache_avg"],
            results["cache_p50"],
            results["cache_p90"],
            results["fully_cold_pct"],
            results["fully_hot_pct"],
            results["lat_cache_corr"],
            results["io_blocks_per_req"],
            results["io_measurement_method"],
            json.dumps(results["entity_p99"]),
            json.dumps(results["entity_p99_contribution_pct"]),
            results["cache_state"],
            results["fetch_mode"],
            results["queries_per_request"],
            results["avg_planning_time_ms"],
            results["avg_rows_per_request"],
            results["avg_payload_bytes"],
            results.get("parallel_workers"),  # V5: worker count
            results.get("latency_per_query_ms"),  # V5: cost-normalized
            json.dumps(results.get("entity_timing_detail", {})),  # V5: Gantt data
            results.get("max_concurrent_queries"),  # V5: actual concurrency
            f"Multi-entity Zipfian benchmark V5 (mode: {results['fetch_mode']}, workers: {results.get('parallel_workers', 'N/A')})"
        ))
    conn.commit()

def persist_query_timings(conn, run_id, request_id, request_idx, fetch_mode, parallel_workers, hot_traffic_pct, query_timings, entity_keys):
    """
    ‚úÖ V5.1: Persist SLOW query samples to log for tail amplification analysis
    
    ONLY logs queries >= SLOW_QUERY_THRESHOLD_MS (default 40ms)
    
    Args:
        conn: Database connection
        run_id: Benchmark run ID
        request_id: Unique request identifier
        request_idx: Request iteration index
        fetch_mode: serial | binpacked | binpacked_parallel
        parallel_workers: Number of workers (or None)
        hot_traffic_pct: Hot traffic percentage
        query_timings: List of SLOW queries {entity, table, latency_ms, rows_returned, was_explain, hash_key, explain_data}
        entity_keys: Dict of entity ‚Üí {hot: [...], cold: [...]} to determine was_hot_key
    """
    if not query_timings:
        return
    
    # ‚úÖ Batch insert for performance (don't commit per query)
    with conn.cursor() as cur:
        for timing in query_timings:
            # Determine if key was hot
            entity = timing["entity"]
            hash_key = timing["hash_key"]
            was_hot_key = hash_key in entity_keys.get(entity, {}).get("hot", [])
            
            # Extract EXPLAIN data if available
            explain_data = timing.get("explain_data", {})
            
            cur.execute(f"""
                INSERT INTO {RESULTS_SCHEMA}.zipfian_slow_query_log (
                    run_id,
                    mode,
                    hot_traffic_pct,
                    iteration_id,
                    request_id,
                    entity_name,
                    table_name,
                    hash_key,
                    was_hot_key,
                    query_latency_ms,
                    rows_returned,
                    explain_used,
                    shared_blks_read,
                    shared_blks_hit,
                    planning_ms,
                    execution_ms
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            """, (
                run_id,
                fetch_mode,
                hot_traffic_pct,
                request_idx,
                request_id,
                timing["entity"],
                timing["table"],
                hash_key,
                was_hot_key,
                timing["latency_ms"],
                timing["rows_returned"],
                timing.get("was_explain", False),
                explain_data.get("shared_blks_read"),
                explain_data.get("shared_blks_hit"),
                explain_data.get("planning_ms"),
                explain_data.get("execution_ms")
            ))
    # ‚úÖ Commit happens in batch (every N requests, handled by caller)

def persist_keys_per_run(conn, run_id, entity_keys):
    """Persist hot/cold keys for this run"""
    with conn.cursor() as cur:
        for entity, keysets in entity_keys.items():
            for key_type, keys in keysets.items():
                for key in keys:
                    cur.execute(
                        f"INSERT INTO {RESULTS_SCHEMA}.{KEYS_TABLE} (run_id, entity_name, key_type, hash_key) VALUES (%s, %s, %s, %s)",
                        (run_id, entity, key_type, key)
                    )
        conn.commit()
    print(f"‚úÖ Persisted {sum(len(v['hot']) + len(v['cold']) for v in entity_keys.values())} keys to {KEYS_TABLE}")

def fetch_keys_from_run(conn, run_id):
    """
    Load hot/cold keys from a previous run
    
    Returns:
        entity_keys: Dict of {entity: {"hot": [...], "cold": [...]}}
        None if run_id not found
    """
    with conn.cursor() as cur:
        # Check if run exists
        cur.execute(f"SELECT COUNT(*) FROM {RESULTS_SCHEMA}.{KEYS_TABLE} WHERE run_id = %s", (run_id,))
        count = cur.fetchone()[0]
        
        if count == 0:
            return None
        
        # Load keys
        cur.execute(
            f"SELECT entity_name, key_type, hash_key FROM {RESULTS_SCHEMA}.{KEYS_TABLE} WHERE run_id = %s",
            (run_id,)
        )
        
        entity_keys = {}
        for entity, key_type, hash_key in cur.fetchall():
            if entity not in entity_keys:
                entity_keys[entity] = {"hot": [], "cold": []}
            entity_keys[entity][key_type].append(hash_key)
        
        return entity_keys

print("‚úÖ Results persistence functions loaded")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 7Ô∏è‚É£ Sample & Persist Keys Per Entity
# MAGIC 
# MAGIC **‚úÖ FIXED: Random sampling from ALL tables**
# MAGIC - Samples keys from ALL tables per entity (not just tables[0])
# MAGIC - Shuffles keys before hot/cold split
# MAGIC - Hot keys are randomly distributed (not first N in storage order)
# MAGIC - Persists keys for reproducibility

# COMMAND ----------

conn = get_conn()

# Ensure results and keys tables exist
ensure_results_table(conn)

# ‚úÖ NEW: Reuse keys from previous run if specified
if REUSE_RUN_ID:
    print(f"üîë Reusing keys from previous run: {REUSE_RUN_ID}")
    print()
    
    t_load_start = time.time()
    entity_keys = fetch_keys_from_run(conn, REUSE_RUN_ID)
    t_load_elapsed = time.time() - t_load_start
    
    if entity_keys is None:
        print(f"   ‚ùå Run ID '{REUSE_RUN_ID}' not found in {KEYS_TABLE}")
        print(f"   ‚ö†Ô∏è  Falling back to sampling new keys...")
        print()
        REUSE_RUN_ID = None  # Force sampling
    else:
        print(f"   ‚úÖ Loaded in {t_load_elapsed:.2f}s")
        for entity, keysets in entity_keys.items():
            print(f"      {entity:20}: Hot: {len(keysets['hot']):>5,} | Cold: {len(keysets['cold']):>5,}")
        print()
        print(f"   ‚è±Ô∏è  Time saved: ~45 minutes (no sampling required)")
        print(f"   üÜî This run will use RUN_ID: {RUN_ID}")
        print()

# Sample new keys if not reusing
if not REUSE_RUN_ID:
    print("üîë Sampling keys per entity (from ALL tables)...")
    print()
    
    entity_keys = {}
    
    for entity, tables in ENTITY_TABLE_GROUPS.items():
        print(f"   Sampling {entity:20}...")
        
        # ‚úÖ FIXED: Sample from ALL tables, not just tables[0]
        keys = []
        keys_per_table = TOTAL_KEYS_PER_ENTITY // len(tables)
        
        for table in tables:
            print(f"      ‚Üí Sampling {table:60}...", end="", flush=True)
            t_start = time.time()
            table_keys = fetch_sample_keys(conn, table, keys_per_table)
            t_elapsed = time.time() - t_start
            keys.extend(table_keys)
            print(f" {len(table_keys):>5} keys in {t_elapsed:.2f}s")
        
        # ‚úÖ FIXED: Shuffle to randomize hot/cold split
        print(f"      ‚Üí Shuffling {len(keys)} keys...")
        random.shuffle(keys)
        
        # Designate hot/cold
        hot_cutoff = int(len(keys) * HOT_KEY_PERCENT_OF_DATASET / 100)
        entity_keys[entity] = {
            "hot": keys[:hot_cutoff],
            "cold": keys[hot_cutoff:]
        }
        
        print(f"      ‚úÖ Total: {len(keys)} keys ‚Üí Hot: {len(entity_keys[entity]['hot']):,} | Cold: {len(entity_keys[entity]['cold']):,}")
        print()
    
    print(f"‚úÖ Key sampling complete!")
    print()
    
    # Persist keys for reproducibility and analysis
    print(f"üíæ Persisting keys to {KEYS_TABLE}...")
    t_persist_start = time.time()
    persist_keys_per_run(conn, RUN_ID, entity_keys)
    t_persist_elapsed = time.time() - t_persist_start
    print(f"   ‚úÖ Persisted in {t_persist_elapsed:.2f}s")
    print(f"   Run ID: {RUN_ID}")
    print(f"   This ensures consistent hot/cold keys across all hot/cold ratios")
    print()

# COMMAND ----------

# MAGIC %md
# MAGIC ## 8Ô∏è‚É£ Feature Fetch Function
# MAGIC 
# MAGIC **‚úÖ FIXED: Serial execution with per-entity timing**
# MAGIC - SELECT * instead of SELECT 1 (fetches actual data)
# MAGIC - Serial execution (no pipelining)
# MAGIC - Per-entity timing (independent measurement)
# MAGIC - Realistic latency measurement

# COMMAND ----------

def fetch_features_for_request(conn, entities_with_keys, iteration_idx=0, sample_io=False, request_start=None, log_query_timings=False):
    """
    ‚úÖ FIXED: Fetch features serially with per-entity timing, optional EXPLAIN sampling
    ‚úÖ V5: Added Gantt chart timing data collection
    ‚úÖ V5.1: Added per-query timing for tail amplification analysis
    
    Args:
        conn: PostgreSQL connection
        entities_with_keys: List of {entity, hashkey, tables}
        iteration_idx: Current iteration number (for sampling)
        sample_io: Whether to sample I/O with EXPLAIN
        request_start: Request start time for Gantt chart (optional)
        log_query_timings: Whether to log per-query timing (V5.1)
    
    Returns:
        entity_timings: Dict of entity ‚Üí latency_ms
        io_blocks: Total disk blocks read (if sampled)
        executor_metrics: Dict with planning_time_ms, rows_returned, payload_size_bytes (if sampled)
        gantt_data: List of {entity, start_ms, end_ms} for Gantt chart (V5)
        query_timings: List of per-query timing data (V5.1)
    """
    entity_timings = {}
    gantt_data = []
    query_timings = []  # ‚úÖ V5.1: Per-query timing
    io_blocks_total = 0
    query_errors = 0
    planning_time_total = 0
    rows_returned_total = 0
    payload_size_bytes_total = 0
    
    # If request_start not provided, use current time
    if request_start is None:
        request_start = time.perf_counter()
    
    with conn.cursor() as cur:
        for entity_info in entities_with_keys:
            # ‚úÖ FIXED: Per-entity timer (not cumulative)
            # ‚úÖ V5: Track absolute timing for Gantt chart
            entity_start = time.perf_counter()
            
            for table in entity_info["tables"]:
                # ‚úÖ V5.1: Always track timing to detect slow queries
                table_start = time.perf_counter()
                
                try:
                    # ‚úÖ FIXED: Optional EXPLAIN sampling for precise I/O
                    explain_data = None
                    if sample_io and iteration_idx % EXPLAIN_SAMPLE_RATE == 0:
                        query = f"EXPLAIN (ANALYZE, BUFFERS) SELECT * FROM {SCHEMA}.{table} WHERE hash_key = %s LIMIT 1"
                        cur.execute(query, (entity_info["hashkey"],))
                        explain_output = cur.fetchall()
                        blocks_read, planning_time_ms, explain_data = parse_buffers_from_explain(explain_output)
                        io_blocks_total += blocks_read
                        planning_time_total += planning_time_ms
                        row = None  # EXPLAIN doesn't return rows
                    else:
                        # ‚úÖ FIXED: SELECT * to fetch actual columns (forces heap read)
                        query = f"SELECT * FROM {SCHEMA}.{table} WHERE hash_key = %s LIMIT 1"
                        cur.execute(query, (entity_info["hashkey"],))
                        row = cur.fetchone()  # Actually fetch the result
                        
                        # Track rows and estimate payload size
                        if row:
                            rows_returned_total += 1
                            # Rough estimate: string length of row representation
                            payload_size_bytes_total += len(str(row))
                    
                    # ‚úÖ V5.1: Log ONLY slow queries (>= threshold)
                    table_end = time.perf_counter()
                    query_latency_ms = (table_end - table_start) * 1000
                    
                    if log_query_timings and query_latency_ms >= SLOW_QUERY_THRESHOLD_MS:
                        # ‚úÖ CRITICAL: Run EXPLAIN on slow queries if we haven't already
                        if not sample_io and not explain_data:
                            try:
                                # Re-run query with EXPLAIN to get I/O details
                                cur.execute(f"EXPLAIN (ANALYZE, BUFFERS) SELECT * FROM {SCHEMA}.{table} WHERE hash_key = %s LIMIT 1", 
                                           (entity_info["hashkey"],))
                                explain_output = cur.fetchall()
                                _, _, explain_data = parse_buffers_from_explain(explain_output)
                            except:
                                explain_data = {}  # If EXPLAIN fails, just skip
                        
                        query_timings.append({
                            "entity": entity_info["entity"],
                            "table": table,
                            "hash_key": entity_info["hashkey"],
                            "latency_ms": query_latency_ms,
                            "rows_returned": 1 if row else 0,
                            "was_explain": sample_io,
                            "explain_data": explain_data or {}
                        })
                
                except Exception as e:
                    # ‚úÖ FIXED: Error handling - log but continue
                    query_errors += 1
                    if query_errors <= 5:  # Only print first 5 errors
                        print(f"         ‚ö†Ô∏è  Query failed: {table[:50]}... | {str(e)[:50]}")
            
            entity_end = time.perf_counter()
            
            # Record per-entity latency
            entity_timings[entity_info["entity"]] = (entity_end - entity_start) * 1000
            
            # ‚úÖ V5: Record Gantt timing data (relative to request start)
            gantt_data.append({
                "entity": entity_info["entity"],
                "start_ms": (entity_start - request_start) * 1000,
                "end_ms": (entity_end - request_start) * 1000
            })
    
    if query_errors > 5:
        print(f"         ‚ö†Ô∏è  ... and {query_errors - 5} more errors")
    
    executor_metrics = {
        "planning_time_ms": planning_time_total if sample_io else None,
        "rows_returned": rows_returned_total if not sample_io else None,
        "payload_size_bytes": payload_size_bytes_total if not sample_io else None
    }
    
    return entity_timings, io_blocks_total, executor_metrics, gantt_data, query_timings

print("‚úÖ Feature fetch function loaded (serial execution, per-entity timing, EXPLAIN sampling, error handling)")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 8Ô∏è‚É£.1 Bin-Packed Fetch Functions (Phase 1)

# COMMAND ----------

def fetch_entity_binpacked(conn, entity, hashkey, tables):
    """
    Fetch all features for one entity using UNION ALL grouped by feature type
    
    ‚úÖ KEY INSIGHT: Within same feature type (fraud_rates, time_since, etc.),
       all time windows (30d, 90d, 365d) have identical schemas!
       
    Strategy: Group tables by feature type, execute one UNION ALL per group
    - fraud_rates__30d/90d/365d ‚Üí 1 UNION ALL query
    - time_since__30d/90d/365d ‚Üí 1 UNION ALL query
    - good_rates__30d/90d/365d ‚Üí 1 UNION ALL query
    
    Reduces 9-12 queries per entity to 3-4 queries! ‚úÖ
    
    Args:
        conn: Database connection
        entity: Entity name
        hashkey: Hash key to query
        tables: List of tables for this entity
    
    Returns:
        List of all results from all feature groups
    """
    from collections import defaultdict
    
    # Group tables by feature type (extract feature name before time window)
    feature_groups = defaultdict(list)
    for table in tables:
        # Extract feature type: "client_id_card_fingerprint__fraud_rates__30d" 
        # ‚Üí "fraud_rates"
        parts = table.split("__")
        if len(parts) >= 2:
            feature_type = parts[-2]  # e.g., "fraud_rates", "time_since", "good_rates"
            feature_groups[feature_type].append(table)
        else:
            feature_groups["other"].append(table)
    
    results = []
    
    with conn.cursor() as cur:
        # Execute one UNION ALL query per feature type
        for feature_type, group_tables in feature_groups.items():
            if len(group_tables) == 1:
                # Single table, no UNION needed
                cur.execute(
                    f"SELECT * FROM {SCHEMA}.{group_tables[0]} WHERE hash_key = %s LIMIT 1",
                    (hashkey,)
                )
                result = cur.fetchall()
                results.extend(result)
            else:
                # Multiple tables with same schema, use UNION ALL
                union_parts = []
                for table in group_tables:
                    union_parts.append(
                        f"(SELECT * FROM {SCHEMA}.{table} WHERE hash_key = %s LIMIT 1)"
                    )
                
                sql = " UNION ALL ".join(union_parts)
                params = [hashkey] * len(group_tables)
                
                cur.execute(sql, params)
                result = cur.fetchall()
                results.extend(result)
    
    return results

def fetch_features_binpacked_serial(conn, entities_with_keys, iteration_idx=0, request_start=None, log_query_timings=False):
    """
    Fetch features using bin-packed queries (3 queries instead of 30), serially
    ‚úÖ V5: Added Gantt chart timing data collection
    ‚úÖ V5.1: Added per-query timing parameter (not implemented for UNION ALL mode)
    
    Args:
        conn: Database connection
        entities_with_keys: List of {entity, hashkey, tables}
        iteration_idx: Current iteration (for logging)
        request_start: Request start time for Gantt chart (optional)
        log_query_timings: Whether to log per-query timing (V5.1, not supported for bin-packed)
    
    Returns:
        entity_timings: Dict of entity ‚Üí latency_ms
        io_blocks: 0 (not tracked in this mode)
        gantt_data: List of {entity, start_ms, end_ms} for Gantt chart (V5)
        query_timings: Empty list (not supported for UNION ALL mode)
    """
    entity_timings = {}
    gantt_data = []
    query_timings = []  # ‚úÖ V5.1: Empty for bin-packed mode (UNION ALL doesn't allow per-table timing)
    
    # If request_start not provided, use current time
    if request_start is None:
        request_start = time.perf_counter()
    
    for entity_info in entities_with_keys:
        entity_start = time.perf_counter()
        
        # Single UNION ALL query for all tables in this entity
        results = fetch_entity_binpacked(
            conn,
            entity_info["entity"],
            entity_info["hashkey"],
            entity_info["tables"]
        )
        
        entity_end = time.perf_counter()
        entity_timings[entity_info["entity"]] = (entity_end - entity_start) * 1000
        
        # ‚úÖ V5: Record Gantt timing data
        gantt_data.append({
            "entity": entity_info["entity"],
            "start_ms": (entity_start - request_start) * 1000,
            "end_ms": (entity_end - request_start) * 1000
        })
    
    return entity_timings, 0, gantt_data, query_timings

# Initialize connection pool for parallel mode
if FETCH_MODE == "binpacked_parallel":
    print("üîó Initializing connection pool for parallel execution...")
    pool = ConnectionPool(
        conninfo=psycopg.conninfo.make_conninfo(**LAKEBASE_CONFIG),
        min_size=10,
        max_size=30,  # ‚úÖ FIX: Larger pool (was 8, caused starvation)
        timeout=10.0,  # ‚úÖ FIX: Pool acquisition timeout (was 30s, too long)
        max_idle=300,  # Recycle idle connections after 5 min
        max_lifetime=1800,  # Force recycle after 30 min
        check=ConnectionPool.check_connection  # ‚úÖ CRITICAL: Health check before handing out
    )
    print(f"   ‚úÖ Pool initialized: min=10, max=30 (with health checks)")
else:
    pool = None

def fetch_entity_worker(entity_info, request_start):
    """
    Worker function for parallel entity fetch
    ‚úÖ V5: Added Gantt chart timing data collection
    
    ‚úÖ FIX: Proper connection lifecycle with instrumentation
    """
    pool_wait_start = time.perf_counter()
    
    # ‚úÖ CRITICAL: Use context manager to ensure connection is returned
    with pool.connection() as conn:
        pool_wait_ms = (time.perf_counter() - pool_wait_start) * 1000
        
        # ‚ö†Ô∏è Log slow pool acquisitions (indicates starvation)
        if pool_wait_ms > 100:
            print(f"         ‚ö†Ô∏è  Slow pool acquisition: {pool_wait_ms:.0f}ms for {entity_info['entity']}")
        
        entity_start = time.perf_counter()
        
        try:
            results = fetch_entity_binpacked(
                conn,
                entity_info["entity"],
                entity_info["hashkey"],
                entity_info["tables"]
            )
            entity_end = time.perf_counter()
            latency_ms = (entity_end - entity_start) * 1000
            
            # ‚úÖ V5: Return Gantt data
            gantt_data = {
                "entity": entity_info["entity"],
                "start_ms": (entity_start - request_start) * 1000,
                "end_ms": (entity_end - request_start) * 1000
            }
            
            return entity_info["entity"], latency_ms, gantt_data
        except Exception as e:
            # ‚úÖ CRITICAL: Ensure connection is returned even on error
            print(f"         ‚ùå Error in {entity_info['entity']}: {str(e)[:100]}")
            raise
    # Connection automatically returned here by context manager

def fetch_features_binpacked_parallel(entities_with_keys, iteration_idx=0, num_workers=3, request_start=None, log_query_timings=False):
    """
    Fetch features using bin-packed queries with parallel execution
    ‚úÖ V5: Added Gantt chart timing data collection
    ‚úÖ V5.1: Added per-query timing parameter (not implemented for UNION ALL mode)
    
    Args:
        entities_with_keys: List of {entity, hashkey, tables}
        iteration_idx: Current iteration (for logging)
        num_workers: Number of parallel workers (V5: configurable)
        request_start: Request start time for Gantt chart (optional)
        log_query_timings: Whether to log per-query timing (V5.1, not supported for bin-packed)
    
    Returns:
        entity_timings: Dict of entity ‚Üí latency_ms
        io_blocks: 0 (not tracked in this mode)
        gantt_data: List of {entity, start_ms, end_ms} for Gantt chart (V5)
        query_timings: Empty list (not supported for UNION ALL mode)
    """
    entity_timings = {}
    gantt_data = []
    query_timings = []  # ‚úÖ V5.1: Empty for bin-packed mode
    
    # If request_start not provided, use current time
    if request_start is None:
        request_start = time.perf_counter()
    
    with ThreadPoolExecutor(max_workers=num_workers) as executor:  # ‚úÖ V5: Parameterized
        futures = [
            executor.submit(fetch_entity_worker, entity_info, request_start)
            for entity_info in entities_with_keys
        ]
        
        for future in futures:
            entity, latency_ms, gantt_entry = future.result()
            entity_timings[entity] = latency_ms
            gantt_data.append(gantt_entry)
    
    return entity_timings, 0, gantt_data, query_timings

print("‚úÖ Bin-packed fetch functions loaded (serial + parallel)")
if pool:
    print("‚úÖ Connection pool ready for parallel mode")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 9Ô∏è‚É£ Run Benchmark (Hot/Cold Matrix)
# MAGIC 
# MAGIC **Multi-Mode Execution:**
# MAGIC - If `run_all_modes=true`: Runs serial ‚Üí binpacked ‚Üí binpacked_parallel sequentially
# MAGIC - Flushes cache between modes for fair comparison
# MAGIC - Uses same hot/cold keys across all modes

# COMMAND ----------

print("\n" + "="*80)
print("üöÄ STARTING MULTI-ENTITY ZIPFIAN BENCHMARK V3")
print("="*80)
print()

if RUN_ALL_MODES:
    mode_list = [f"{cfg['mode']}" + (f" (w={cfg['workers']})" if cfg['workers'] else "") for cfg in MODE_CONFIGS]
    print(f"üìã Running ALL modes sequentially: {mode_list}")
    print(f"   Cache flush between each mode")
else:
    print(f"üìã Running single mode: {FETCH_MODE}")
print()

# Store all results across all modes
all_results = {}
random.seed(42)  # Reproducible results

# ‚úÖ V5: Loop through each mode configuration (mode + workers)
for mode_idx, mode_config in enumerate(MODE_CONFIGS):
    current_mode = mode_config["mode"]
    current_workers = mode_config["workers"]
    
    print("\n" + "="*80)
    mode_label = get_mode_label(current_mode, current_workers)
    print(f"üéØ CONFIG {mode_idx + 1}/{len(MODE_CONFIGS)}: {mode_label}")
    print("="*80)
    if current_workers:
        print(f"   Workers: {current_workers}")
    print()
    
    # Initialize connection pool for parallel mode
    if current_mode == "binpacked_parallel":
        pool_max_size = current_workers * 10  # ‚úÖ V5: Scale pool with workers
        print(f"üîó Initializing connection pool ({current_workers} workers)...")
        pool = ConnectionPool(
            conninfo=psycopg.conninfo.make_conninfo(**LAKEBASE_CONFIG),
            min_size=min(current_workers * 2, 10),
            max_size=pool_max_size,  # ‚úÖ V5: Dynamic pool sizing
            timeout=10.0,
            max_idle=300,
            max_lifetime=1800,
            check=ConnectionPool.check_connection
        )
        print(f"   ‚úÖ Pool initialized: max={pool_max_size} (workers={current_workers})")
        print()
    else:
        pool = None
    
    # Flush cache before this mode (except for first mode)
    if mode_idx > 0:
        print("üîÑ Flushing cache before starting this mode...")
        flush_cache(conn)
        # Close and reopen connection to clear client-side state
        conn.close()
        conn = get_conn()
        print("   ‚úÖ Connection refreshed")
        print()
    
    results = {}
    
    for hot_pct in HOT_COLD_MATRIX:
        cold_pct = 100 - hot_pct
        print("\n" + "-"*80)
        print(f"   {hot_pct}% HOT / {cold_pct}% COLD (per entity)")
        print("-"*80)
        
        # Reset PostgreSQL stats for accurate I/O measurement
        reset_pg_stats(conn)
        io_before_reads, io_before_hits = read_pg_io_stats(conn)
        
        latencies = []
        cache_scores = []
        entity_latency = defaultdict(list)
        io_blocks_sampled = 0
        io_sample_count = 0
        sample_io = False  # ‚úÖ FIX Bug 2: Initialize sample_io for all modes
        planning_time_sampled = 0
        rows_returned_total = 0
        payload_size_total = 0
        gantt_samples = []  # ‚úÖ V5: Collect Gantt samples (first 20 iterations)
        GANTT_SAMPLE_SIZE = 20
        slow_query_count = 0  # ‚úÖ V5.1: Track number of slow queries logged
        SLOW_QUERY_BATCH_SIZE = 100  # ‚úÖ V5.1: Commit every N slow query inserts
        
        # Benchmark loop
        for i in range(ITERATIONS_PER_RUN):
            # ‚úÖ CRITICAL: Refresh connection every 100 iterations to prevent timeouts
            if i > 0 and i % 100 == 0:
                conn = refresh_connection_if_stale(conn)
            
            # Verbose logging for first 3 iterations to debug hangs
            if i < 3:
                print(f"         ‚Üí Iteration {i+1}: Building request...")
            
            t0 = time.time()
            hot_entities = 0
            entities_for_request = []
            
            # Build multi-entity request
            for entity in ENTITY_NAMES:
                tables = ENTITY_TABLE_GROUPS[entity]
                keyset = entity_keys[entity]
                
                # Each entity independently chooses hot or cold
                if random.random() < hot_pct / 100:
                    hashkey = random.choice(keyset["hot"])
                    hot_entities += 1
                else:
                    hashkey = random.choice(keyset["cold"])
                
                entities_for_request.append({
                    "entity": entity,
                    "hashkey": hashkey,
                    "tables": tables
                })
            
            if i < 3:
                print(f"            Built request with {len(entities_for_request)} entities, {hot_entities} hot")
                print(f"            Fetching features (mode: {current_mode})...")
            
            # Dispatch to correct fetch strategy based on current_mode
            # ‚úÖ CRITICAL: Retry on connection errors
            max_retries = 3
            request_start = time.perf_counter()  # ‚úÖ V5: Track request start for Gantt
            
            # ‚úÖ V5.1: Generate unique request_id for tail amplification analysis
            request_id = f"{RUN_ID}_{current_mode}_{hot_pct}_{i}_{uuid.uuid4().hex[:8]}"
            log_timings = LOG_QUERY_TIMINGS  # Always track, but only log slow queries
            
            for attempt in range(max_retries):
                try:
                    if current_mode == "serial":
                        # Original serial execution (30 queries)
                        sample_io = (i % EXPLAIN_SAMPLE_RATE == 0)
                        entity_timings, io_blocks, executor_metrics, gantt_data, query_timings = fetch_features_for_request(conn, entities_for_request, i, sample_io, request_start, log_timings)
                        
                        # Aggregate executor metrics
                        if sample_io and io_blocks > 0:
                            io_blocks_sampled += io_blocks
                            io_sample_count += 1
                            if executor_metrics["planning_time_ms"]:
                                planning_time_sampled += executor_metrics["planning_time_ms"]
                        
                        if not sample_io:
                            if executor_metrics["rows_returned"]:
                                rows_returned_total += executor_metrics["rows_returned"]
                            if executor_metrics["payload_size_bytes"]:
                                payload_size_total += executor_metrics["payload_size_bytes"]
                        
                        # ‚úÖ V5.1: Persist slow query timings
                        if log_timings and query_timings:
                            persist_query_timings(conn, RUN_ID, request_id, i, current_mode, current_workers, hot_pct, query_timings, entity_keys)
                            slow_query_count += len(query_timings)
                            
                            # ‚úÖ Batch commit every N slow queries
                            if slow_query_count >= SLOW_QUERY_BATCH_SIZE:
                                conn.commit()
                                if i < 3:
                                    print(f"            ‚úÖ Committed {slow_query_count} slow queries")
                                slow_query_count = 0
                                
                    elif current_mode == "binpacked":
                        # Bin-packed serial (10 UNION ALL queries)
                        entity_timings, io_blocks, gantt_data, query_timings = fetch_features_binpacked_serial(conn, entities_for_request, i, request_start, log_timings)
                    elif current_mode == "binpacked_parallel":
                        # Bin-packed parallel (10 queries, N workers) ‚úÖ V5: Parameterized workers
                        entity_timings, io_blocks, gantt_data, query_timings = fetch_features_binpacked_parallel(entities_for_request, i, current_workers, request_start, log_timings)
                    
                    # ‚úÖ V5: Collect Gantt samples for visualization
                    if i < GANTT_SAMPLE_SIZE:
                        gantt_samples.append({
                            "iteration": i,
                            "hot_entities": hot_entities,
                            "mode": current_mode,
                            "workers": current_workers,
                            "entities": gantt_data
                        })
                    
                    break  # Success - exit retry loop
                    
                except (psycopg.OperationalError, psycopg.InterfaceError) as e:
                    if attempt < max_retries - 1:
                        print(f"         ‚ö†Ô∏è  Connection error (attempt {attempt + 1}/{max_retries}): {str(e)[:100]}")
                        conn = refresh_connection_if_stale(conn)
                        time.sleep(1)  # Brief pause before retry
                    else:
                        print(f"         ‚ùå Connection failed after {max_retries} attempts")
                        raise
            
            if i < 3:
                print(f"            Completed in {(time.time()-t0)*1000:.1f}ms")
            
            # Track per-entity latencies
            for entity, timing_ms in entity_timings.items():
                entity_latency[entity].append(timing_ms)
            
            # ‚úÖ FIX Bug #2: Calculate latency correctly based on execution mode
            if current_mode == "binpacked_parallel":
                # Parallel execution: critical path is MAX(entity latencies)
                latency_ms = max(entity_timings.values()) if entity_timings else 0
            else:
                # Serial execution: total latency is SUM(entity latencies)
                latency_ms = sum(entity_timings.values()) if entity_timings else 0
            
            latencies.append(latency_ms)
            
            # Cache score: fraction of entities that were hot (0.0 = all cold, 1.0 = all hot)
            cache_scores.append(hot_entities / len(ENTITY_NAMES))
            
            # ‚úÖ FIXED: Percentage-based progress printing
            if (i + 1) % max(1, ITERATIONS_PER_RUN // 10) == 0:
                pct = ((i + 1) / ITERATIONS_PER_RUN) * 100
                recent_avg = np.mean(latencies[-100:]) if len(latencies) >= 100 else np.mean(latencies)
                print(f"         Progress: {pct:.0f}% ({i+1}/{ITERATIONS_PER_RUN}) | Recent avg: {recent_avg:.1f}ms")
    
        # Read I/O stats (aggregate across all queries - for comparison)
        io_after_reads, io_after_hits = read_pg_io_stats(conn)
        io_blocks_read_aggregate = io_after_reads - io_before_reads
        
        # ‚úÖ FIX Bug 3: Label I/O measurement based on mode
        if current_mode == "serial":
            # Calculate I/O per request (prefer EXPLAIN-sampled data if available)
            if io_sample_count > 0:
                io_blocks_per_req = io_blocks_sampled / io_sample_count
                io_measurement_method = "EXPLAIN sampled"
            else:
                io_blocks_per_req = io_blocks_read_aggregate / ITERATIONS_PER_RUN if ITERATIONS_PER_RUN > 0 else 0
                io_measurement_method = "pg_statio aggregate"
        else:
            # Bin-packed modes: I/O not measured (would require EXPLAIN on UNION ALL)
            io_blocks_per_req = None
            io_measurement_method = "not_measured"
        
        # Calculate statistics
        lat = np.array(latencies)
        cache = np.array(cache_scores)
        
        # Calculate per-entity P99s and contribution %
        entity_p99_dict = {}
        entity_p99_contribution = {}
        total_entity_p99 = 0
        
        for entity, timings in entity_latency.items():
            if len(timings) > 0:
                p99 = float(np.percentile(timings, 99))
                entity_p99_dict[entity] = p99
                total_entity_p99 += p99
        
        # ‚úÖ NEW: Calculate per-entity P99 contribution %
        for entity, p99 in entity_p99_dict.items():
            entity_p99_contribution[entity] = (p99 / total_entity_p99 * 100) if total_entity_p99 > 0 else 0
        
        # ‚úÖ FIXED: Handle NaN in correlation for edge cases (0% or 100% hot)
        if cache.std() > 0 and lat.std() > 0:
            lat_cache_corr = float(np.corrcoef(lat, cache)[0, 1])
        else:
            lat_cache_corr = 0.0  # No variance = no correlation
        
        # Determine cache state label
        if mode_idx == 0:
            cache_state = "best_effort_cold"  # First mode after no warmup
        elif hot_pct >= 80:
            cache_state = "warm"
        elif hot_pct <= 30:
            cache_state = "mixed_cold"
        else:
            cache_state = "mixed"
        
        # ‚úÖ FIX Bug #3: Calculate executor metrics averages correctly
        non_explain_iters = ITERATIONS_PER_RUN - io_sample_count
        
        avg_planning_time_ms = (planning_time_sampled / io_sample_count) if io_sample_count > 0 else None
        avg_rows_per_request = (rows_returned_total / non_explain_iters) if non_explain_iters > 0 else None
        avg_payload_bytes = (payload_size_total / non_explain_iters) if non_explain_iters > 0 else None
        
        # ‚úÖ V5: Calculate queries per request (updated for binpacked UNION ALL)
        queries_per_request = 30 if current_mode == "serial" else 10  # V5: 10 UNION ALL queries
        
        results[hot_pct] = {
            "p50": np.percentile(lat, 50),
            "p95": np.percentile(lat, 95),
            "p99": np.percentile(lat, 99),
            "avg": lat.mean(),
            "cache_avg": cache.mean(),
            "cache_p50": np.percentile(cache, 50),
            "cache_p90": np.percentile(cache, 90),
            "fully_cold_pct": (cache == 0).mean() * 100,
            "fully_hot_pct": (cache == 1).mean() * 100,
            "lat_cache_corr": lat_cache_corr,
            "io_blocks_per_req": io_blocks_per_req,
            "io_measurement_method": io_measurement_method,
            "entity_p99": entity_p99_dict,
            "entity_p99_contribution_pct": entity_p99_contribution,
            "cache_state": cache_state,
            "fetch_mode": current_mode,
            "queries_per_request": queries_per_request,
            "avg_planning_time_ms": avg_planning_time_ms,
            "avg_rows_per_request": avg_rows_per_request,
            "avg_payload_bytes": avg_payload_bytes,
            # ‚úÖ V5: New metrics
            "parallel_workers": current_workers,
            "latency_per_query_ms": lat.mean() / queries_per_request,  # Cost-normalized!
            "entity_timing_detail": gantt_samples,  # ‚úÖ V5: Gantt chart data (first 20 iterations)
            "max_concurrent_queries": current_workers if current_mode == "binpacked_parallel" else None
        }
        
        # ‚úÖ V5.1: Commit any remaining slow queries
        total_slow_queries_logged = sum(len(timing) for timing in gantt_samples if "query_timings" in timing)  # Rough estimate
        if slow_query_count > 0:
            conn.commit()
            print(f"         ‚úÖ Final commit: {slow_query_count} slow queries")
        
        if LOG_QUERY_TIMINGS:
            # Count slow queries by scanning what we logged
            conn_temp = get_conn()
            with conn_temp.cursor() as cur:
                cur.execute(f"""
                    SELECT COUNT(*) 
                    FROM {RESULTS_SCHEMA}.zipfian_slow_query_log 
                    WHERE run_id = %s AND mode = %s AND hot_traffic_pct = %s
                """, (RUN_ID, current_mode, hot_pct))
                slow_count = cur.fetchone()[0]
            conn_temp.close()
            print(f"         üêå Slow queries logged: {slow_count} (>= {SLOW_QUERY_THRESHOLD_MS}ms)")
        
        # Persist results
        persist_results(conn, hot_pct, results[hot_pct])
        
        # Print summary
        print()
        print(f"         üìä Results:")
        print(f"            P99 latency:             {results[hot_pct]['p99']:.1f} ms")
        print(f"            Avg cache score:         {results[hot_pct]['cache_avg']:.2f}")
        print(f"            Fully hot requests:      {results[hot_pct]['fully_hot_pct']:.1f}%")
        print(f"            Fully cold requests:     {results[hot_pct]['fully_cold_pct']:.1f}%")
        
        # ‚úÖ FIX: Clear messaging when I/O not measured
        if io_blocks_per_req is not None:
            print(f"            I/O blocks/req:          {io_blocks_per_req:.1f} ({io_measurement_method})")
        else:
            print(f"            I/O blocks/req:          N/A (not measured for bin-packed modes)")
        
        print(f"            Entity P99 contributions:")
        for entity, contrib in entity_p99_contribution.items():
            print(f"               {entity:25} {entity_p99_dict[entity]:.1f}ms ({contrib:.1f}%)")
    
    # Store results for this configuration (V5: unique key per mode+workers)
    config_key = f"{current_mode}_w{current_workers}" if current_workers else current_mode
    all_results[config_key] = results
    
    # Close connection pool if it was created
    if pool:
        print()
        print("üîó Closing connection pool...")
        pool.close()
        pool = None
    
    print()
    print("="*80)
    print(f"‚úÖ MODE COMPLETE: {current_mode}")
    print("="*80)
    print()

conn.close()

print("\n" + "="*80)
print("‚úÖ ALL BENCHMARKS COMPLETE!")
print("="*80)
print()

if RUN_ALL_MODES or len(MODE_CONFIGS) > 1:
    print("üìä CONFIGURATIONS EXECUTED:")
    for cfg in MODE_CONFIGS:
        label = get_mode_label(cfg["mode"], cfg["workers"])
        print(f"   ‚úÖ {label}")
    print()
    print("üíæ All results stored in features.zipfian_feature_serving_results_v5")
    print(f"   Run ID: {RUN_ID}")
    print(f"   Filter by fetch_mode column to compare")
else:
    print(f"üìä MODE EXECUTED: {FETCH_MODE}")
    print(f"üíæ Results stored with fetch_mode = '{FETCH_MODE}'")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üîü Results Summary Table

# COMMAND ----------

print("\n" + "="*100)
print("üìä FINAL BENCHMARK MATRIX SUMMARY")
print("="*100)
print()

# Print results for each mode configuration
for config_key in all_results.keys():
    mode_results = all_results[config_key]  # ‚úÖ FIX Bug #1: Don't shadow 'results'
    
    # Extract mode name and workers for label
    if '_w' in config_key:
        base_mode = config_key.split('_w')[0]
        workers_str = config_key.split('_w')[1]
        mode_label = f"{FETCH_MODE_LABELS.get(base_mode, base_mode)} ({workers_str} workers)"
    else:
        base_mode = config_key
        mode_label = FETCH_MODE_LABELS.get(base_mode, config_key)
    
    print(f"\nüéØ CONFIG: {config_key.upper()} - {mode_label}")
    print("-"*100)
    
    print(
        f"{'Hot%':>6} | {'P50(ms)':>8} | {'P95(ms)':>8} | {'P99(ms)':>8} | {'Avg(ms)':>8} | "
        f"{'CacheAvg':>8} | {'ColdReq%':>8} | {'HotReq%':>7} | {'LatCache':>8}"
    )
    print("-"*100)
    
    for hot_pct, r in sorted(mode_results.items(), reverse=True):
        print(
            f"{hot_pct:>6} | "
            f"{r['p50']:>8.1f} | "
            f"{r['p95']:>8.1f} | "
            f"{r['p99']:>8.1f} | "
            f"{r['avg']:>8.1f} | "
            f"{r['cache_avg']:>8.2f} | "
            f"{r['fully_cold_pct']:>8.1f} | "
            f"{r['fully_hot_pct']:>7.1f} | "
            f"{r['lat_cache_corr']:>8.2f}"
        )

print()
print("="*100)
print()
print(f"üíæ Results persisted to: {RESULTS_SCHEMA}.{RESULTS_TABLE} (run_id: {RUN_ID})")
print(f"üìä Query to compare modes:")
print(f"""
SELECT fetch_mode, hot_traffic_pct, p99_ms, avg_ms, queries_per_request
FROM {RESULTS_SCHEMA}.{RESULTS_TABLE}
WHERE run_id = '{RUN_ID}'
ORDER BY fetch_mode, hot_traffic_pct DESC;
""")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1Ô∏è‚É£1Ô∏è‚É£ Visualizations
# MAGIC 
# MAGIC **Note:** Visualizing the last mode executed. For multi-mode comparison, query the results table directly.

# COMMAND ----------

# ‚úÖ FIX Bug #1: Use explicit mode for visualizations (last config if multi-mode)
viz_config_key = list(all_results.keys())[-1] if RUN_ALL_MODES else (f"{FETCH_MODE}_w{PARALLEL_WORKERS[0]}" if FETCH_MODE == "binpacked_parallel" else FETCH_MODE)
viz_results = all_results[viz_config_key]

print(f"üìä Visualizing results for config: {viz_config_key.upper()}")
print()

fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 1. P99 Latency vs Hot Traffic %
ax1 = axes[0, 0]
hot_pcts = sorted(viz_results.keys(), reverse=True)
p99_values = [viz_results[h]['p99'] for h in hot_pcts]

ax1.plot(hot_pcts, p99_values, 'o-', linewidth=2, markersize=8, color='#2E86AB')
ax1.axhline(79, color='red', linestyle='--', linewidth=2, label='DynamoDB P99 (79ms)')
ax1.set_xlabel('Hot Traffic % (per entity)', fontsize=12)
ax1.set_ylabel('P99 Latency (ms)', fontsize=12)
ax1.set_title('P99 Latency vs Hot Traffic %', fontsize=14, weight='bold')
ax1.grid(True, alpha=0.3)
ax1.legend()
ax1.invert_xaxis()

# 2. Cache Score vs Hot Traffic %
ax2 = axes[0, 1]
cache_avg = [viz_results[h]['cache_avg'] for h in hot_pcts]
cache_p90 = [viz_results[h]['cache_p90'] for h in hot_pcts]

ax2.plot(hot_pcts, cache_avg, 'o-', linewidth=2, markersize=8, label='Avg Cache Score', color='#A23B72')
ax2.plot(hot_pcts, cache_p90, 's--', linewidth=2, markersize=6, label='P90 Cache Score', color='#F18F01')
ax2.set_xlabel('Hot Traffic % (per entity)', fontsize=12)
ax2.set_ylabel('Cache Score (0=all cold, 1=all hot)', fontsize=12)
ax2.set_title('Cache Effectiveness vs Hot Traffic %', fontsize=14, weight='bold')
ax2.grid(True, alpha=0.3)
ax2.legend()
ax2.invert_xaxis()

# 3. Request Distribution (Fully Hot vs Fully Cold)
ax3 = axes[1, 0]
fully_hot = [viz_results[h]['fully_hot_pct'] for h in hot_pcts]
fully_cold = [viz_results[h]['fully_cold_pct'] for h in hot_pcts]

width = 3
x = np.array(hot_pcts)
ax3.bar(x - width/2, fully_hot, width, label='Fully Hot (all 3 entities)', color='#C73E1D', alpha=0.7)
ax3.bar(x + width/2, fully_cold, width, label='Fully Cold (all 3 entities)', color='#4ECDC4', alpha=0.7)
ax3.set_xlabel('Hot Traffic % (per entity)', fontsize=12)
ax3.set_ylabel('% of Requests', fontsize=12)
ax3.set_title('Request Distribution: Fully Hot vs Fully Cold', fontsize=14, weight='bold')
ax3.grid(True, alpha=0.3, axis='y')
ax3.legend()
ax3.invert_xaxis()

# 4. Latency-Cache Correlation
ax4 = axes[1, 1]
lat_cache_corr = [viz_results[h]['lat_cache_corr'] for h in hot_pcts]

ax4.plot(hot_pcts, lat_cache_corr, 'o-', linewidth=2, markersize=8, color='#6A4C93')
ax4.set_xlabel('Hot Traffic % (per entity)', fontsize=12)
ax4.set_ylabel('Correlation Coefficient', fontsize=12)
ax4.set_title('Latency ‚Üî Cache Correlation', fontsize=14, weight='bold')
ax4.grid(True, alpha=0.3)
ax4.axhline(0, color='black', linestyle='-', linewidth=0.5)
ax4.invert_xaxis()

plt.tight_layout()
plt.savefig('/tmp/zipfian_multi_entity_benchmark_v3.png', dpi=150, bbox_inches='tight')
print("üìä Visualization saved to /tmp/zipfian_multi_entity_benchmark_v3.png")
plt.show()

# COMMAND ----------

# MAGIC %md
# MAGIC ## ‚úÖ Key Insights

# COMMAND ----------

print("\n" + "="*80)
print("üí° KEY INSIGHTS")
print("="*80)
print(f"Config: {viz_config_key.upper()}")
print("="*80)
print()

# Find 80% hot case (closest match)
target_hot = 80
closest_hot = min(viz_results.keys(), key=lambda x: abs(x - target_hot))
r80 = viz_results[closest_hot]

print(f"üéØ REALISTIC PRODUCTION SCENARIO ({closest_hot}% hot traffic per entity):")
print(f"   P99 latency:           {r80['p99']:.1f} ms")
print(f"   Avg cache score:       {r80['cache_avg']:.2f} (0=all cold, 1=all hot)")
print(f"   Fully hot requests:    {r80['fully_hot_pct']:.1f}%  (all 3 entities cached)")
print(f"   Fully cold requests:   {r80['fully_cold_pct']:.1f}%  (all 3 entities on disk)")
print(f"   Mixed requests:        {100 - r80['fully_hot_pct'] - r80['fully_cold_pct']:.1f}%  (1-2 entities cached)")
print()

# Calculate expected cache score for 80% hot per entity
expected_all_hot = 0.8 ** 3 * 100  # All 3 entities hot
expected_all_cold = 0.2 ** 3 * 100  # All 3 entities cold
print(f"üìä STATISTICAL VALIDATION:")
print(f"   Expected fully hot:    {expected_all_hot:.1f}%  (0.8¬≥)")
print(f"   Actual fully hot:      {r80['fully_hot_pct']:.1f}%")
print(f"   Expected fully cold:   {expected_all_cold:.1f}%  (0.2¬≥)")
print(f"   Actual fully cold:     {r80['fully_cold_pct']:.1f}%")
print(f"   ‚úÖ Matches expected distribution!")
print()

print(f"üèÜ COMPARISON TO DYNAMODB:")
print(f"   DynamoDB P99:          79.0 ms")
print(f"   Lakebase P99:          {r80['p99']:.1f} ms")
if r80['p99'] < 79:
    improvement = ((79 - r80['p99']) / 79) * 100
    print(f"   ‚úÖ Lakebase WINS by {improvement:.1f}%!")
elif r80['p99'] < 95:
    print(f"   ‚úÖ Competitive (within 20% of DynamoDB)")
else:
    print(f"   ‚ö†Ô∏è  Slower than DynamoDB (optimization needed)")
print()

print(f"üí∞ COST:")
print(f"   Lakebase:              ~$100/day")
print(f"   DynamoDB:              $75,000/day (50 tables √ó $1,500/day)")
print(f"   Savings:               $27M/year (750x cheaper)")
print()

print("="*80)
print(f"‚úÖ Benchmark complete! Run ID: {RUN_ID}")
print(f"üìä View results: SELECT * FROM {RESULTS_SCHEMA}.{RESULTS_TABLE} WHERE run_id = '{RUN_ID}';")
print("="*80)
