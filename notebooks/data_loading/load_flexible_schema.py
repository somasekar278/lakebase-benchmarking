# Databricks notebook source
# MAGIC %md
# MAGIC # Load Flexible Schema - N Tables with M Features
# MAGIC 
# MAGIC This notebook loads data for a flexible schema generated by `flexible_schema_generator.py`.
# MAGIC 
# MAGIC **Configuration:**
# MAGIC - Set `NUM_TABLES`, `FEATURES_PER_TABLE`, and `ROWS_PER_TABLE` in the configuration cell
# MAGIC - Or pass via job parameters: `num_tables`, `features_per_table`, `rows_per_table`
# MAGIC 
# MAGIC **Usage:**
# MAGIC - Interactive: Run all cells
# MAGIC - Job: Pass table_index parameter to load specific table

# COMMAND ----------

# MAGIC %md ## Configuration

# COMMAND ----------

# MAGIC %pip install psycopg2-binary

# COMMAND ----------

dbutils.library.restartPython()

# COMMAND ----------

import psycopg2
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, expr, md5, concat_ws, rand, when
from pyspark.sql.types import *
import time

# Lakebase connection details
LAKEBASE_HOST = 'ep-autumn-fire-d318blbk.database.eu-west-1.cloud.databricks.com'
LAKEBASE_PORT = 5432
LAKEBASE_DATABASE = 'benchmark'
LAKEBASE_USER = 'fraud_benchmark_user'
LAKEBASE_PASSWORD = 'fraud_benchmark_user_123!'

# Schema configuration - CUSTOMIZE THESE
NUM_TABLES = 30  # Number of tables to create
FEATURES_PER_TABLE = 5  # Features per table
ROWS_PER_TABLE = 10_000_000  # Rows per table (start with 10M for testing)

# Get from job parameters if running as a job
try:
    NUM_TABLES = int(dbutils.widgets.get("num_tables"))
    FEATURES_PER_TABLE = int(dbutils.widgets.get("features_per_table"))
    ROWS_PER_TABLE = int(dbutils.widgets.get("rows_per_table"))
    TABLE_INDEX = int(dbutils.widgets.get("table_index"))  # Specific table to load
    print(f"üìã Job parameters: {NUM_TABLES} tables, {FEATURES_PER_TABLE} features, {ROWS_PER_TABLE:,} rows")
    print(f"   Loading table {TABLE_INDEX}")
except:
    TABLE_INDEX = None  # Load all tables if not specified
    print(f"üìã Configuration: {NUM_TABLES} tables, {FEATURES_PER_TABLE} features, {ROWS_PER_TABLE:,} rows")
    print(f"   Loading ALL tables")

TOTAL_FEATURES = NUM_TABLES * FEATURES_PER_TABLE
print(f"üìä Total features: {TOTAL_FEATURES}")

# COMMAND ----------

# MAGIC %md ## Helper Functions

# COMMAND ----------

def get_table_name(table_idx):
    """Get table name for given index"""
    return f"feature_table_{table_idx:02d}"

def get_feature_names(table_idx):
    """Generate feature names for a table"""
    # Simple feature naming: feature_1_t00, feature_2_t00, etc.
    return [f"feature_{i+1}_t{table_idx:02d}" for i in range(FEATURES_PER_TABLE)]

def generate_table_data(table_idx, num_rows):
    """
    Generate synthetic data for a table using Spark DataFrame.
    
    Uses parallel generation for speed.
    """
    print(f"\n{'='*70}")
    print(f"Generating data for table {table_idx}: {get_table_name(table_idx)}")
    print(f"{'='*70}")
    
    table_name = get_table_name(table_idx)
    feature_names = get_feature_names(table_idx)
    
    # Calculate key offset to ensure unique primary keys across tables
    key_offset = table_idx * ROWS_PER_TABLE
    
    print(f"   Rows: {num_rows:,}")
    print(f"   Features: {len(feature_names)}")
    print(f"   Key offset: {key_offset:,}")
    
    # Start with a range DataFrame
    df = spark.range(num_rows)
    
    # Generate primary key (SHA256 hash)
    # Offset by table_idx to ensure uniqueness across tables
    df = df.withColumn(
        "primary_key",
        expr(f"sha2(CAST(id + {key_offset} AS STRING), 256)")
    )
    
    # Generate raw_fingerprint
    df = df.withColumn(
        "raw_fingerprint",
        concat_ws("_", lit("fingerprint"), col("id"))
    )
    
    # Generate feature columns with random numeric values
    for feature_name in feature_names:
        # Mix of different value ranges for realism
        df = df.withColumn(
            feature_name,
            when(rand() < 0.3, (rand() * 100).cast("decimal(18,6)"))  # 0-100
            .when(rand() < 0.6, (rand() * 1000).cast("decimal(18,6)"))  # 0-1000
            .otherwise((rand() * 10000).cast("decimal(18,6)"))  # 0-10000
        )
    
    # Generate updated_at timestamp
    df = df.withColumn(
        "updated_at",
        (expr("unix_timestamp()") - (rand() * 86400 * 365).cast("long"))  # Random time in last year
    )
    
    # Drop the temporary id column
    df = df.drop("id")
    
    print(f"   ‚úÖ Generated {df.count():,} rows")
    
    return df

def load_table_to_lakebase(df, table_name):
    """
    Load DataFrame to Lakebase using JDBC with optimizations.
    """
    print(f"\n{'='*70}")
    print(f"Loading to Lakebase: features.{table_name}")
    print(f"{'='*70}")
    
    jdbc_url = f"jdbc:postgresql://{LAKEBASE_HOST}:{LAKEBASE_PORT}/{LAKEBASE_DATABASE}"
    
    connection_properties = {
        "user": LAKEBASE_USER,
        "password": LAKEBASE_PASSWORD,
        "driver": "org.postgresql.Driver",
        "batchsize": "50000",
        "isolationLevel": "READ_UNCOMMITTED",
        "tcpKeepAlive": "true",
        "socketTimeout": "600",
        "connectTimeout": "60"
    }
    
    # Clear existing data first (idempotent)
    print(f"   üóëÔ∏è  Truncating table...")
    conn = psycopg2.connect(
        host=LAKEBASE_HOST,
        port=LAKEBASE_PORT,
        database=LAKEBASE_DATABASE,
        user=LAKEBASE_USER,
        password=LAKEBASE_PASSWORD
    )
    cursor = conn.cursor()
    cursor.execute(f"TRUNCATE TABLE features.{table_name}")
    conn.commit()
    conn.close()
    print(f"   ‚úÖ Table truncated")
    
    # Write data
    start_time = time.time()
    print(f"   üì§ Writing {df.count():,} rows...")
    
    # Use more partitions for larger tables
    num_partitions = min(200, max(50, df.count() // 100000))
    
    df.repartition(num_partitions).write \
        .format("jdbc") \
        .option("url", jdbc_url) \
        .option("dbtable", f"features.{table_name}") \
        .option("user", LAKEBASE_USER) \
        .option("password", LAKEBASE_PASSWORD) \
        .option("driver", "org.postgresql.Driver") \
        .option("batchsize", "50000") \
        .option("isolationLevel", "READ_UNCOMMITTED") \
        .option("tcpKeepAlive", "true") \
        .option("socketTimeout", "600") \
        .option("connectTimeout", "60") \
        .mode("append") \
        .save()
    
    elapsed = time.time() - start_time
    rows_per_sec = df.count() / elapsed
    
    print(f"   ‚úÖ Load complete!")
    print(f"   ‚è±Ô∏è  Time: {elapsed:.1f}s ({rows_per_sec:,.0f} rows/sec)")
    
    # Verify
    conn = psycopg2.connect(
        host=LAKEBASE_HOST,
        port=LAKEBASE_PORT,
        database=LAKEBASE_DATABASE,
        user=LAKEBASE_USER,
        password=LAKEBASE_PASSWORD
    )
    cursor = conn.cursor()
    cursor.execute(f"SELECT COUNT(*) FROM features.{table_name}")
    count = cursor.fetchone()[0]
    conn.close()
    
    print(f"   ‚úÖ Verified: {count:,} rows in database")
    
    if count != df.count():
        print(f"   ‚ö†Ô∏è  WARNING: Expected {df.count():,} rows, got {count:,}")
    
    return count

# COMMAND ----------

# MAGIC %md ## Load Data

# COMMAND ----------

print("=" * 70)
print("FLEXIBLE SCHEMA DATA LOADING")
print("=" * 70)
print(f"Configuration:")
print(f"  - Tables: {NUM_TABLES}")
print(f"  - Features per table: {FEATURES_PER_TABLE}")
print(f"  - Rows per table: {ROWS_PER_TABLE:,}")
print(f"  - Total features: {TOTAL_FEATURES}")
print(f"  - Total rows: {NUM_TABLES * ROWS_PER_TABLE:,}")
print("=" * 70)

# Determine which tables to load
if TABLE_INDEX is not None:
    # Load specific table (for job parallelization)
    tables_to_load = [TABLE_INDEX]
    print(f"\nüìã Job mode: Loading table {TABLE_INDEX} only")
else:
    # Load all tables (interactive mode)
    tables_to_load = range(NUM_TABLES)
    print(f"\nüìã Interactive mode: Loading all {NUM_TABLES} tables")

# Load tables
results = []

for table_idx in tables_to_load:
    table_name = get_table_name(table_idx)
    
    print(f"\n{'='*70}")
    print(f"Processing table {table_idx+1}/{NUM_TABLES}: {table_name}")
    print(f"{'='*70}")
    
    try:
        # Generate data
        df = generate_table_data(table_idx, ROWS_PER_TABLE)
        
        # Load to Lakebase
        row_count = load_table_to_lakebase(df, table_name)
        
        results.append({
            'table_index': table_idx,
            'table_name': table_name,
            'rows': row_count,
            'status': 'SUCCESS'
        })
        
        print(f"\n‚úÖ Table {table_idx} complete: {row_count:,} rows")
        
    except Exception as e:
        print(f"\n‚ùå Table {table_idx} failed: {e}")
        results.append({
            'table_index': table_idx,
            'table_name': table_name,
            'rows': 0,
            'status': f'FAILED: {str(e)[:100]}'
        })

# COMMAND ----------

# MAGIC %md ## Summary

# COMMAND ----------

print("\n" + "=" * 70)
print("LOAD SUMMARY")
print("=" * 70)

successful = sum(1 for r in results if r['status'] == 'SUCCESS')
failed = len(results) - successful
total_rows = sum(r['rows'] for r in results)

print(f"\nüìä Results:")
print(f"   ‚úÖ Successful: {successful}/{len(results)} tables")
print(f"   ‚ùå Failed: {failed}/{len(results)} tables")
print(f"   üìà Total rows loaded: {total_rows:,}")

print(f"\nüìã Per-table results:")
for r in results:
    status_emoji = "‚úÖ" if r['status'] == 'SUCCESS' else "‚ùå"
    print(f"   {status_emoji} {r['table_name']}: {r['rows']:,} rows - {r['status']}")

if failed > 0:
    print(f"\n‚ö†Ô∏è  {failed} table(s) failed. Check logs above for details.")
else:
    print(f"\nüéâ All tables loaded successfully!")

print("\n" + "=" * 70)

# COMMAND ----------

# MAGIC %md ## Next Steps
# MAGIC 
# MAGIC After loading all tables:
# MAGIC 
# MAGIC 1. **Create stored procedure:**
# MAGIC    ```bash
# MAGIC    psql -f generated/setup_flexible_stored_proc.sql
# MAGIC    ```
# MAGIC 
# MAGIC 2. **Run benchmark:**
# MAGIC    ```python
# MAGIC    python notebooks/benchmarks/benchmark_flexible.py
# MAGIC    ```

# COMMAND ----------



