# Jobs configuration for Databricks Asset Bundles
# Fraud feature benchmarking jobs

resources:
  jobs:
    # ALL 30 TABLES: 16.6B rows total
    fraud_all_tables:
      name: "[${bundle.target}] Full Load: All 30 Tables (16.6B rows)"
      description: "Load all 30 fraud feature tables with correct row counts"
      
      tasks:
        - task_key: generate_csvs
          description: "Task 1: Generate CSVs for all 30 tables"
          existing_cluster_id: "0120-120746-h595w64t"
          
          notebook_task:
            notebook_path: ../notebooks/generate_csvs.py
            base_parameters:
              lakebase_host: ${var.lakebase_host}
              lakebase_database: ${var.lakebase_database}
              lakebase_schema: ${var.lakebase_schema}
              lakebase_user: ${var.lakebase_user}
              lakebase_password: ${var.lakebase_password}
              ddl_file_path: "${workspace.file_path}/generated/fraud_feature_tables.sql"
              rows_per_table_file: "${workspace.file_path}/generated/fraud_tables_row_counts.txt"
              uc_volume_path: "/Volumes/${var.catalog}/${var.schema}/${var.volume}"
              csv_only: "true"
          
          timeout_seconds: 86400  # 24 hours
          max_retries: 0
        
        - task_key: pipelined_load
          description: "Task 2: Pipelined Load (COPY → INDEX → SWAP)"
          depends_on:
            - task_key: generate_csvs
          existing_cluster_id: "0120-120746-h595w64t"
          
          notebook_task:
            notebook_path: ../notebooks/run_pipelined_load.py
            base_parameters:
              lakebase_host: ${var.lakebase_host}
              lakebase_database: ${var.lakebase_database}
              lakebase_schema: ${var.lakebase_schema}
              lakebase_user: ${var.lakebase_user}
              lakebase_password: ${var.lakebase_password}
              uc_volume_path: "/Volumes/${var.catalog}/${var.schema}/${var.volume}"
          
          timeout_seconds: 86400  # 24 hours
          max_retries: 0
      
      max_concurrent_runs: 1
      
      tags:
        Environment: ${bundle.target}
        Project: fraud-benchmarking
        Type: production
      
      email_notifications:
        on_failure:
          - ${workspace.current_user.userName}
        on_success:
          - ${workspace.current_user.userName}
    
    # ZIPFIAN MULTI-ENTITY BENCHMARK WITH VISUALIZATIONS (V5.4 - Production Grade + RPC Mode)
    zipfian_benchmark_full:
      name: "[${bundle.target}] Zipfian Multi-Entity Benchmark V5.4 + RPC + Visualizations"
      description: "Run production-grade multi-entity Zipfian benchmark (100% → 0% hot traffic) with all 4 modes including RPC"
      
      tasks:
        - task_key: get_last_run_id
          description: "Task 0: Get Latest Run ID for Key Reuse"
          existing_cluster_id: "0120-120746-h595w64t"
          
          notebook_task:
            notebook_path: ../get_last_run_id.py
            base_parameters:
              dbhost: ${var.lakebase_host}
              dbport: "5432"
          
          max_retries: 0
        
        - task_key: run_zipfian_benchmark
          description: "Task 1: Run Multi-Entity Zipfian Benchmark V5.4 (9 hot/cold ratios, 4 modes)"
          depends_on:
            - task_key: get_last_run_id
          existing_cluster_id: "0120-120746-h595w64t"
          
          notebook_task:
            notebook_path: ../notebooks/benchmark_zipfian_realistic_v5.4.py
            base_parameters:
              lakebase_host: ${var.lakebase_host}
              lakebase_database: ${var.lakebase_database}
              lakebase_schema: ${var.lakebase_schema}
              lakebase_user: ${var.lakebase_user}
              lakebase_password: ${var.lakebase_password}
              iterations_per_run: "1000"
              total_keys_per_entity: "10000"
              hot_key_percent: "1"
              run_all_modes: "true"
              fetch_mode: "serial"
              reuse_run_id: "PLACEHOLDER_SET_MANUALLY"
              parallel_workers: "1,2,3,4"
              log_query_timings: "true"
              slow_query_threshold_ms: "40"
          
          libraries:
            - pypi:
                package: psycopg[binary,pool]
            - pypi:
                package: numpy
            - pypi:
                package: pandas
            - pypi:
                package: matplotlib
            - pypi:
                package: seaborn
          
          # No timeout - let it run as long as needed
          max_retries: 0
        
        - task_key: generate_visualizations
          description: "Task 2: Generate 7 Professional Visualizations"
          depends_on:
            - task_key: run_zipfian_benchmark
          existing_cluster_id: "0120-120746-h595w64t"
          
          notebook_task:
            notebook_path: ../notebooks/zipfian_benchmark_visuals.py
            base_parameters:
              lakebase_host: ${var.lakebase_host}
              lakebase_database: ${var.lakebase_database}
              lakebase_user: ${var.lakebase_user}
              lakebase_password: ${var.lakebase_password}
              run_id: "latest"
          
          libraries:
            - pypi:
                package: psycopg[binary,pool]
            - pypi:
                package: numpy
            - pypi:
                package: pandas
            - pypi:
                package: matplotlib
            - pypi:
                package: seaborn
            - pypi:
                package: plotly
            - pypi:
                package: kaleido
          
          # No timeout - let it run as long as needed
          max_retries: 0
      
      max_concurrent_runs: 1
      timeout_seconds: 0  # CRITICAL: No timeout - let benchmark run as long as needed
      
      tags:
        Environment: ${bundle.target}
        Project: fraud-benchmarking
        Type: benchmark
      
      email_notifications:
        on_failure:
          - ${workspace.current_user.userName}
        on_success:
          - ${workspace.current_user.userName}
    
    # RPC MODE MINI TEST (V5.4) - Quick 10-iteration validation
    rpc_mode_test:
      name: "[${bundle.target}] RPC Mode Mini Test (10 iterations)"
      description: "Quick test of rpc_request_json mode with 10 iterations + visualization"
      
      tasks:
        - task_key: run_rpc_benchmark
          description: "Task 1: Run RPC Mode Benchmark (10 iterations)"
          existing_cluster_id: "0120-120746-h595w64t"
          
          notebook_task:
            notebook_path: ../notebooks/benchmark_zipfian_realistic_v5.4.py
            base_parameters:
              lakebase_host: ${var.lakebase_host}
              lakebase_database: ${var.lakebase_database}
              lakebase_schema: ${var.lakebase_schema}
              lakebase_user: ${var.lakebase_user}
              lakebase_password: ${var.lakebase_password}
              fetch_mode: "rpc_request_json"
              run_all_modes: "false"
              iterations_per_run: "10"
              hot_key_percent: "1"
              log_query_timings: "false"
          
          libraries:
            - pypi:
                package: psycopg[binary,pool]
            - pypi:
                package: numpy
            - pypi:
                package: pandas
            - pypi:
                package: matplotlib
            - pypi:
                package: seaborn
          
          max_retries: 0
        
        - task_key: generate_visualizations
          description: "Task 2: Generate Visualizations with RPC Mode"
          depends_on:
            - task_key: run_rpc_benchmark
          existing_cluster_id: "0120-120746-h595w64t"
          
          notebook_task:
            notebook_path: ../notebooks/zipfian_benchmark_visuals.py
            base_parameters:
              lakebase_host: ${var.lakebase_host}
              lakebase_database: ${var.lakebase_database}
              lakebase_user: ${var.lakebase_user}
              lakebase_password: ${var.lakebase_password}
              run_id: "latest"
              results_table: "v5"
          
          libraries:
            - pypi:
                package: psycopg[binary,pool]
            - pypi:
                package: numpy
            - pypi:
                package: pandas
            - pypi:
                package: matplotlib
            - pypi:
                package: seaborn
            - pypi:
                package: plotly
            - pypi:
                package: kaleido
            - pypi:
                package: jinja2
          
          max_retries: 0
      
      max_concurrent_runs: 1
      
      tags:
        Environment: ${bundle.target}
        Project: fraud-benchmarking
        Type: test
        Mode: rpc_request_json
      
      email_notifications:
        on_failure:
          - ${workspace.current_user.userName}
        on_success:
          - ${workspace.current_user.userName}
    
    # CSV BACKUP JOB - Run before workspace wipeout
    csv_backup:
      name: "[${bundle.target}] CSV Backup to DBFS"
      description: "Backup all CSVs from UC Volume to DBFS for download"
      
      tasks:
        - task_key: backup_csvs
          description: "Compress and backup CSVs from UC Volume"
          existing_cluster_id: "0120-120746-h595w64t"
          
          notebook_task:
            notebook_path: ../notebooks/backup_csvs_from_volume.py
          
          timeout_seconds: 7200  # 2 hours
          max_retries: 0
      
      max_concurrent_runs: 1
      
      tags:
        Environment: ${bundle.target}
        Project: fraud-benchmarking
        Type: backup
      
      email_notifications:
        on_failure:
          - ${workspace.current_user.userName}
        on_success:
          - ${workspace.current_user.userName}
